---
title: "Optimal Hyperparameters and model averaging Analysis with Corrected ELBO"
output: pdf_document
header-includes:
   - \usepackage{bbm, amsthm}
toc: T
---
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}

# Overview

In this document, I present the results a grid search and model averaging experiment conducted across 100 trials after correcting the ELBO. I first analyze the distribution of the optimal hyperparameters selected by grid search. Next, I compare the results of model averaging to the grid search approach. 

The first section shows the correction made to the ELBO. The following two main sections contain these analyses, while the fourth section describes the data and extraneous covariate generation.

# ELBO Calculations

Let $(y_i, x_i)$ be independent response-covariate pairs, with $y_i\in\mathbb R, x_i\in\mathbb R^p$. Consider the following model:

\begin{align}
(\beta,\gamma) & \sim \prod_{k = 1}^p\delta_{\{0\}}(\beta_k)^{1-\gamma_k}\mathcal N\left(\beta_k;0,\sigma^2\sigma^2_\beta\right)^{\gamma_k}\hat{\pi}^{\gamma_k}(1-\hat{\pi})^{1-\gamma_k}\\
y_i|\beta&\sim \mathcal N\left(x_i^\top\beta,\frac{\sigma^2}{w_i}\right)
\end{align}

Approximate $[\beta, \gamma|y]\propto[y_i|\beta][\beta,\gamma]$ with $q$:

\begin{equation}
q(\beta,\gamma) = \prod_{k = 1}^p\delta_{\{0\}}(\beta_k)^{1-\gamma_k}\mathcal N\left(\beta_k;\mu_k,s_k^2\right)^{\gamma_k}\alpha_k^{\gamma_k}(1-\alpha_k)^{1-\gamma_k}
\end{equation}

Then, the ELBO is given by:

\begin{equation}
\mathbb E_q(\log[\beta,\gamma]) + \mathbb E_q(\log[Y|\beta]) - \mathbb E_q[\log q(\beta,\gamma)]
\end{equation}

The corrected ELBO as currently in the code is:

\begin{align}
\mathbb E_q(\log[\beta,\gamma])  & = \sum_{k = 1}^p\left(-\frac{\alpha_k}2\log(\sigma^2\sigma^2_\beta) - \frac{\alpha_k(s_k^2 + \mu^2_k)}{2\sigma^2\sigma^2_\beta} + \alpha_k\log(\hat\pi)+ (1-\alpha_k)\log(1 - \hat\pi)\right)
\\
+\quad\quad & 
\\
\mathbb E_q(\log[Y|\beta]) & = -\frac1{2\sigma^2}\sum_{i = 1}^nw_i\left(\left\{y_i - \sum_{k = 1}^px_{ik}\mu_k\alpha_k\right\}^2 + \sum_{k=1}^px_{ik}^2(\alpha_k(\mu_k^2 + s_k^2) - \alpha_k^2\mu_k^2)\right) - \frac n2\log(2\pi\sigma^2)
\\
-\quad\quad & 
\\
\mathbb E_q[\log q(\beta,\gamma)] & = \sum_{k = 1}^p\left(-\frac{\alpha_k}2\log(s_k^2)-\frac{\alpha_k}2 + \alpha_k\log(\alpha_k) + (1-\alpha_k)\log(1-\alpha_k)\right)
\end{align}

Prior to this correction, the last term in $\mathbb E_q(\log[Y|\beta])$ was $\frac 12\log(2\pi\sigma^2)$ instead of $\frac n2\log(2\pi\sigma^2)$.

Note that the expression for $\mathbb E_q(\log[Y|\beta])$ is missing a $\frac12\sum_{i = 1}^n\log(w_i)$. However, since the $w_i$ are constant, the omission of this term does not affect the final model. 

# Optimal Pi 

## Hyperparameter specification

```{r, echo = F}
library(ggplot2)
library(ggpubr)
library(covdepGE)


load("importance_sampling3_submods.Rda")

# # find counts of total number of edges
# tot_edges <- sapply(lapply(lapply(lapply(lapply(
#   res, `[[`, "out"), "[[", "unique_graphs"), lapply, `[[`, "graph"), sapply, sum), sum)
# 
# # find counts on graph with max number of edges
# max_edges <- sapply(lapply(lapply(lapply(
#   lapply(res, `[[`, "out"), "[[", "unique_graphs"), lapply, `[[`, "graph"), sapply, sum), max)
# 
# plot(res[[order(max_edges, tot_edges, decreasing = T)[1]]]$out)
# 
# plot(res[[which.max(tot_edges)]]$out)

# optimal pi analysis

# get all the models
mods <- lapply(res, `[[`, "out_grid")

# recover the marginal hyperparameter grids
marg_grids <- lapply(res[[1]]$out_grid$hyperparameters$`Variable 1`$grid[ , c("pip", "ssq", "sbsq")], unique)
```

In this experiment, I applied the grid search algorithm in 100 trials. I generated the grid from the cartesian product of the following marginal grids:

```{r}
# marginal grids
marg_grids

# number of grid points in the cartesian product
nrow(expand.grid(marg_grids))
```

## Optimal Hyperparameter Distribution

In this section, I display the distribution of optimal hyperparameters aggregated across all variables. Since a unique point in the hyperparameter grid was selected for each of the 5 variables, a total of 500 grid points are represented in each figure. 

I begin by displaying the true underlying precision structures to provide context for the optimal pi. 

```{r, echo = F, fig.width = 11, fig.height = 9}
set.seed(1)

# function for generating the data and the covariates
generate_continuous <- function(n1 = 60, n2 = 60, n3 = 60, p = 4){

  # create covariate for individuals in each of the three intervals

  # define the dimensions of the data
  n <- sum(n1, n2, n3)

  # define the limits of the intervals
  limits1 <- c(-3, -1)
  limits2 <- c(-1, 1)
  limits3 <- c(1, 3)

  # define the covariate values within each interval
  z1 <- runif(n1, limits1[1], limits1[2])
  z2 <- runif(n2, limits2[1], limits2[2])
  z3 <- runif(n3, limits3[1], limits3[2])
  Z <- matrix(sort(c(z1, z2, z3)), n, 1)

  # create precision matrices

  # the shared part of the structure for all three intervals is a 2 on the
  # diagonal and a 1 in the (2, 3) position
  common_str <- diag(p + 1)
  common_str[2, 3] <- 1

  # define constants for the structure of interval 2
  beta1 <- diff(limits2)^-1
  beta0 <- -limits2[1] * beta1

  # interval 2 has two different linear functions of Z in the (1, 2) position
  # and (1, 3) positions; define structures for each of these components
  int2_str12 <- int2_str13 <- matrix(0, p + 1, p + 1)
  int2_str12[1, 2] <- int2_str13[1, 3] <- 1

  # define the precision matrices for each of the individuals in interval 2
  int2_prec <- lapply(z2, function(z) common_str +
                        ((1 - beta0 - beta1*z)*int2_str12) +
                        ((beta0 + beta1*z)*int2_str13))

  # interval 1 has a 1 in the (1, 2) and interval 3 has a 1 in the (1, 3) position;
  # define structures for each of these components
  int1_str12 <- int3_str13 <- matrix(0, p + 1, p + 1)
  int1_str12[1, 2] <- int3_str13[1, 3] <- 1

  # define the precision matrices for each of the individuals in interval 1 and interval 3
  int1_prec <- rep(list(common_str + int1_str12), n1)
  int3_prec <- rep(list(common_str + int3_str13), n3)

  # put all of the precision matrices into one list
  prec_mats <- c(int1_prec, int2_prec, int3_prec)

  # symmetrize the precision matrices
  prec_mats <- lapply(prec_mats, function(mat) t(mat) + mat)

  # invert the precision matrices to get the covariance matrices
  cov_mats <- lapply(prec_mats, solve)

  # generate the data using the covariance matrices
  data_mat <- t(sapply(cov_mats, MASS::mvrnorm, n = 1, mu = rep(0, p + 1)))

  return(list(data = data_mat, covts = Z, true_precision = prec_mats))
}

# visualize the true precision structures
true_prec <- generate_continuous()$true_precision
true_graphs <- unique(lapply(true_prec, function(mat) (mat != 0) * 1 - diag(5)))
gr_viz <- lapply(true_graphs, gg_adjMat)
ggarrange(gr_viz[[1]] + ggtitle("Individuals 1,...,60"), 
          gr_viz[[2]] + ggtitle("Individuals 61,...,120"),
          gr_viz[[3]] + ggtitle("Individuals 121,...,180"))
```
```{r, echo = F, fig.width = 11, fig.height = 6}
# get all of the hyperparameter details from models
hyp <- lapply(mods, `[[`, "hyperparameters")

# get the optimal pi for each of the trials by variable
pi_opt <- lapply(hyp, lapply, `[[`, "pip")

# visualize the distribution of the optimal pi across all variables
pi_vec <- unlist(pi_opt)
pi_df <- cbind.data.frame(variable = names(pi_vec), pi = factor(pi_vec, levels = marg_grids$pip))
ggplot(pi_df, aes(pi, fill = variable)) + geom_bar(color = "black") + scale_x_discrete(drop = F) + 
  ggtitle("Distribution of Optimal Pi") +
  theme(axis.text.x = element_text(angle = 45, hjust=1))

# get the optimal sbsq for each of the trials by variable
sbsq_opt <- lapply(hyp, lapply, `[[`, "sbsq")

# visualize the distribution of the optimal sbsq across all variables
sbsq_vec <- unlist(sbsq_opt)
sbsq_df <- cbind.data.frame(variable = names(sbsq_vec), sbsq = factor(sbsq_vec, levels = marg_grids$sbsq))
ggplot(sbsq_df, aes(sbsq, fill = variable)) + geom_bar(color = "black") + scale_x_discrete(drop = F) + 
  ggtitle("Distribution of Optimal Sigmabeta_sq") +
  theme(axis.text.x = element_text(angle = 45, hjust=1))

# get the optimal ssq for each of the trials by variable
ssq_opt <- lapply(hyp, lapply, `[[`, "ssq")

# visualize the distribution of the optimal ssq across all variables
ssq_vec <- unlist(ssq_opt)
ssq_df <- cbind.data.frame(variable = names(ssq_vec), ssq = factor(ssq_vec, levels = marg_grids$ssq))
ggplot(ssq_df, aes(ssq, fill = variable)) + geom_bar(color = "black") + scale_x_discrete(drop = F)+ 
  ggtitle("Distribution of Optimal Sigma_sq") +
  theme(axis.text.x = element_text(angle = 45, hjust=1))
```

## Optimal Sigmabeta_sq as a function of Pi

Here, I visualize the optimal sigmabeta_sq as a function of pi. Note that the count is on the log scale. 

```{r, echo = F, fig.width = 11, fig.height = 6}
# visualize the optimal sbsq as a function of pi
(pi_tab <- table(pi_df$pi, sbsq_df$sbsq))
pitab_lg <- data.frame(expand.grid(
  pi = rownames(pi_tab), sbsq = colnames(pi_tab)), count = c(pi_tab))
pitab_lg$log_ct <- ifelse(pitab_lg$count == 0, 0, log(pitab_lg$count))
ggplot(pitab_lg, aes(pi, sbsq, fill = log_ct)) + 
  geom_tile(color = "black") + 
  scale_fill_gradient(low = "white") + 
  theme(axis.text.x = element_text(angle = 45, hjust=1))
# ggplot() + geom_jitter(aes(pi_vec, sbsq_vec), shape = 21, width = 0.005, height = 0.001, fill = "#80000075")
```

## Optimal Pi by variable

In this section, I visualize the optimal pi by variable. 

```{r, fig.width = 11, fig.height = 6, echo = F}
# visualize the optimal pi by variable
var_names <- names(hyp[[1]])
lapply(var_names, function(var_name) ggplot() +
         geom_bar(aes(factor(sapply(
           pi_opt, `[[`, var_name), levels = marg_grids$pip)), color = "black") +
         scale_x_discrete(drop = F) + xlab("pi") + ggtitle(var_name) + 
         theme(axis.text.x = element_text(angle = 45, hjust=1)))
```

## $p = 20$

In this section, I present the same analyses as above for a dataset with $p =20$.

```{r, echo = F}
load("opt_pi2_20var_corelbo.Rda")
# optimal pi analysis

# get all the models
mods <- lapply(res, `[[`, "out")

# recover the marginal hyperparameter grids
marg_grids <- apply(res[[1]]$out$hyperparameters$`Variable 1`$grid[ , 1:3], 2,
                    unique)
marg_grids <- as.data.frame(marg_grids)
```

The marginal grids are:

```{r}
# marginal grids
marg_grids

# number of grid points in the cartesian product
nrow(expand.grid(marg_grids))
```

The true precision structures are given below:

```{r, echo = F, fig.width = 11, fig.height = 9}
set.seed(1)

# visualize the true precision structures
true_prec20 <- generate_continuous(p = 20 - 1)$true_precision
true_graphs <- unique(lapply(true_prec20, function(mat) (mat != 0) * 1 - diag(20)))
gr_viz <- lapply(true_graphs, gg_adjMat)
ggarrange(gr_viz[[1]] + ggtitle("Individuals 1,...,60"),
          gr_viz[[2]] + ggtitle("Individuals 61,...,120"),
          gr_viz[[3]] + ggtitle("Individuals 121,...,180"))
```
Below is the distribution of the optimal hyperparameters aggregated across all variables.

```{r, echo = F, fig.width = 11, fig.height = 6}
# get all of the hyperparameter details from models
hyp <- lapply(mods, `[[`, "hyperparameters")

# get the optimal pi for each of the trials by variable
pi_opt <- lapply(hyp, lapply, `[[`, "pip")

# visualize the distribution of the optimal pi across all variables
pi_vec <- unlist(pi_opt)
pi_df <- cbind.data.frame(variable = names(pi_vec), pi = factor(pi_vec, levels = marg_grids$pip))
pi_df$variable <- factor(pi_df$variable)
varname_ord <- order(as.numeric(sapply(strsplit(
  levels(pi_df$variable), " "), `[[`, 2)))
pi_df$variable <- factor(pi_df$variable, levels(pi_df$variable)[varname_ord])
ggplot(pi_df, aes(pi, fill = variable)) + geom_bar(color = "black") + scale_x_discrete(drop = F) +
  ggtitle("Distribution of Optimal Pi") +
  theme(axis.text.x = element_text(angle = 45, hjust=1))

# get the optimal sbsq for each of the trials by variable
sbsq_opt <- lapply(hyp, lapply, `[[`, "sbsq")

# visualize the distribution of the optimal sbsq across all variables
sbsq_vec <- unlist(sbsq_opt)
sbsq_df <- cbind.data.frame(variable = names(sbsq_vec), sbsq = factor(sbsq_vec, levels = marg_grids$sbsq))
sbsq_df$variable <- factor(sbsq_df$variable)
varname_ord <- order(as.numeric(sapply(strsplit(
  levels(sbsq_df$variable), " "), `[[`, 2)))
sbsq_df$variable <- factor(sbsq_df$variable, levels(sbsq_df$variable)[varname_ord])
ggplot(sbsq_df, aes(sbsq, fill = variable)) + geom_bar(color = "black") + scale_x_discrete(drop = F) +
  ggtitle("Distribution of Optimal Sigmabeta_sq")

# get the optimal ssq for each of the trials by variable
ssq_opt <- lapply(hyp, lapply, `[[`, "ssq")

# visualize the distribution of the optimal ssq across all variables
ssq_vec <- unlist(ssq_opt)
ssq_df <- cbind.data.frame(variable = names(ssq_vec), ssq = factor(ssq_vec, levels = marg_grids$ssq))
ssq_df$variable <- factor(ssq_df$variable)
varname_ord <- order(as.numeric(sapply(strsplit(
  levels(ssq_df$variable), " "), `[[`, 2)))
ssq_df$variable <- factor(ssq_df$variable, levels(ssq_df$variable)[varname_ord])
ggplot(ssq_df, aes(ssq, fill = variable)) + geom_bar(color = "black") + scale_x_discrete(drop = F)+
  ggtitle("Distribution of Optimal Sigma_sq")
```

The optimal value of sbsq as a function of pi is visualized below.

```{r, echo = F, fig.width = 11, fig.height = 6}
# visualize the optimal sbsq as a function of pi
(pi_tab <- table(pi_df$pi, sbsq_df$sbsq))
pitab_lg <- data.frame(expand.grid(
  pi = rownames(pi_tab), sbsq = colnames(pi_tab)), count = c(pi_tab))
pitab_lg$log_ct <- ifelse(pitab_lg$count == 0, 0, log(pitab_lg$count))
ggplot(pitab_lg, aes(pi, sbsq, fill = log_ct)) +
  geom_tile(color = "black") +
  scale_fill_gradient(low = "white") +
  theme(axis.text.x = element_text(angle = 45, hjust=1))
# ggplot() + geom_jitter(aes(pi_vec, sbsq_vec), shape = 21, width = 0.005, height = 0.001, fill = "#80000075")
```

Here the distribution of the optimal pi is visualized by variable.

```{r, echo = F, fig.height = 12, fig.width = 8}
# visualize the optimal pi by variable
var_names <- names(hyp[[1]])
pi_dist20 <- lapply(var_names, function(var_name) ggplot() +
         geom_bar(aes(factor(sapply(
           pi_opt, `[[`, var_name), levels = marg_grids$pip)), color = "black") +
         scale_x_discrete(drop = F) + xlab("pi") + ggtitle(var_name) +
         theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 4)))
#for (j in 1:length(pi_dist20)) plot(pi_dist20[[j]])
#plot(pi_dist20[[1]])
ggarrange(plotlist = pi_dist20, nrow = 5, ncol = 4)
```


# Model Averaging

## Algorithm Overview

This section describes the model averaging approach used here for a spike-and-slab regression of $y\in\mathbb R^n$ on $X\in\mathbb R^{n\times p}$ to compute an importance-weighted-average of the posterior inclusion probabilities $P(\gamma_k = 1|y, X, \theta_j)$, where $\gamma_k = 1 \iff \beta_k = 1$.

Denote the importance weights by $\underset\sim w = w(\theta_1),..., w(\theta_N)$, where $\theta_1,..., \theta_N$ are points the points comprising the hyperparameter grid.

With the $j$-th column of the data as the response $y$ and the data with the $j$-th column removed as the covariates $X$, an extraneous-covariate-weighted regression will be performed with respect to individual $l$ for each $\theta\in\underset\sim\theta$. The importance-weighted sum of the posterior quantities $p(\gamma_k = 1|X, y, \theta_i)$ will then be computed using:

\begin{equation}
w(\theta_i) = \frac{p(y|X, \theta_i)p(\theta_i)}{q(\theta_i)}
\end{equation}

Where $p(y|X, \theta_i), p(\theta_i)$, and $q(\theta_i)$ are the marginal log-likelihood, the prior density, and the importance density, respectively. The prior density and the importance density are defined as:

\begin{equation}
p(\theta_i) = p({\sigma^2}_i) * p({\sigma^2_\beta}_i) * p({\pi}_i) \quad\quad q(\theta_i) = q({\sigma^2}_i) * q({\sigma^2_\beta}_i) * q({\pi}_i)
\end{equation}

<!-- In the case of `prior`, $p({\sigma^2_\beta}_i)$ should actually be written hierarchically as $p({\sigma^2_\beta}_i|{\pi}_i)$. Also, in both `prior` and `unif_prior`, $q(\theta_i)$ is the uniform density on the hyperparameter grid. Then, the weighted importance sum coverges almost surely to the posterior probability of inclusion: -->

\begin{proposition}\label{as_pip}
Let $w(\theta_i) = p(y|X, \theta_i)$. Then:
\begin{equation}
\lim_{N\to\infty}\frac{\sum_{i = 1}^N p(\gamma_k = 1|X, y, \theta_i)w(\theta_i)}{\sum_{i = 1}^N w(\theta_i)} = p(\gamma_k = 1|X, y), \textit{ a.s.}
\end{equation}
\end{proposition}

The approximation of $p(y|X, \theta_i)$ is made using $\exp(\textit{ELBO}_j^l(\theta_i))$, where $\textit{ELBO}_j^l(\theta_i)$ is the log-lower bound of the marginal likelihood $p(y|X,\theta_i)$ in the extraneous-covariate-weighted regression of $y$ on $X$ with respect to individual $l$.

Note that the prior and importance density used in this experiment were uniform, which is equivalent to:

\begin{equation}
\frac{\sum_{i = 1}^N p(\gamma_k = 1|X, y, \theta_i)p(y|X, \theta_i)}{\sum_{i = 1}^N p(y|X, \theta_i)}
\end{equation}

## Sensitivity and Specificity

In this section, I compare the sensitivity and the specificity for the two methods, grid search and model averaging.

```{r, echo = F}
load("importance_sampling3_submods.Rda")

# get the true graphs
true_graphs <- lapply(true_prec, function(mat) (mat != 0) * 1 - diag(5))

# calculate total number of edges in the true graphs
num1 <- sum(unlist(true_graphs))
num0 <- sum(unlist(true_graphs) == 0)

# get the importance and grid mods
impt_mods <- lapply(res, `[[`, "out_impt")
grid_mods <- lapply(res, `[[`, "out_grid")

# calculate sensitivity and specificity for models
impt1 <- sapply(1:length(impt_mods), function(trial_ind)
  sum(sapply(1:180, function(indv_ind)
    sum(impt_mods[[trial_ind]]$graphs[[indv_ind]] == 1 & true_graphs[[indv_ind]] == 1))))
impt0 <- sapply(1:length(impt_mods), function(trial_ind)
  sum(sapply(1:180, function(indv_ind)
    sum(impt_mods[[trial_ind]]$graphs[[indv_ind]] == 0 & true_graphs[[indv_ind]] == 0))))
impt_sens <- impt1 / num1
impt_spec <- impt0 / num0
grid1 <- sapply(1:length(grid_mods), function(trial_ind)
  sum(sapply(1:180, function(indv_ind)
    sum(grid_mods[[trial_ind]]$graphs[[indv_ind]] == 1 & true_graphs[[indv_ind]] == 1))))
grid0 <- sapply(1:length(grid_mods), function(trial_ind)
  sum(sapply(1:180, function(indv_ind)
    sum(grid_mods[[trial_ind]]$graphs[[indv_ind]] == 0 & true_graphs[[indv_ind]] == 0))))
grid_sens <- grid1 / num1
grid_spec <- grid0 / num0
```

```{r, fig.width = 11, fig.height = 6}
# grid sensitivity
summary(grid_sens)

# importance sensitivity
summary(impt_sens)

# difference in the sensitivities
summary(grid_sens - impt_sens)

# visualize the difference in the sensitivity
ggplot() + geom_histogram(aes(grid_sens - impt_sens), binwidth = 0.025, color = "black")

# grid specificity
summary(grid_spec)

# importance specificity
summary(impt_spec)

# difference in the specificities
summary(impt_spec - grid_spec)

# visualize the difference in the specificity
ggplot() + geom_histogram(aes(impt_spec - grid_spec), binwidth = 0.025, color = "black")
```

## Case Study

Here, I present the graphs estimated by the two methods in two cases; the first case represents the performance of the methods in the trial wherein grid search outperformed model averaging by the greatest margin in terms of sensitivity. The second case represents the performance of the methods in the trial wherein model averaging outperformed grid search by the greatest margin in terms of specificity. 

### Case 1: Greatest Grid Search Margin of Victory for Sensitivity

```{r, fig.width = 11, fig.height = 9}
diff_ind <- which.max(grid_sens - impt_sens)

# sensitivity for grid search
grid_sens[diff_ind]

# sensitivity for model averaging
impt_sens[diff_ind]

# specificity for grid search
grid_spec[diff_ind]

# specificity for model averaging
impt_spec[diff_ind]

# graphs estimated by grid search
ggarrange(plotlist = plot(grid_mods[[diff_ind]]))

# graphs estimated by model averaging
ggarrange(plotlist = plot(impt_mods[[diff_ind]]))
```

### Case 2: Greatest Model Averaging Margin of Victory for Specificity

```{r, fig.width = 11, fig.height = 12}
diff_ind <- which.max(impt_spec - grid_spec)

# sensitivity for grid search
grid_sens[diff_ind]

# sensitivity for model averaging
impt_sens[diff_ind]

# specificity for grid search
grid_spec[diff_ind]

# specificity for model averaging
impt_spec[diff_ind]

# graphs estimated by grid search
ggarrange(plotlist = plot(grid_mods[[diff_ind]]), ncol = 3, nrow = 4)
```

```{r, fig.width = 11, fig.height = 9}
# graphs estimated by model averaging
ggarrange(plotlist = plot(impt_mods[[diff_ind]]))
```

# Data Generation

## Extraneous Covariate

I generated the covariate, $Z$, as the union of three almost disjoint intervals of equal measure. That is, $Z = Z_1 \cup Z_2 \cup Z_3$ with $Z_1 = (-3, -1), Z_2 = (a,b) = (-1, 1), Z_3 = (1, 3)$. Within each interval, I generated 60 covariate values from a uniform distribution. For example:

```{r, echo = F, fig.width = 11, fig.height = 4.5}
cont <- generate_continuous()
X <- cont$data
Z <- cont$covts
n <- nrow(X)
p <- ncol(X) - 1

interval <- c(rep(1, 60), rep(2, 60), rep(3, 60))
cov_df <- cbind.data.frame(interval = interval, Z, individual_index = 1:n)
cov_df$interval <- factor(cov_df$interval)
ggplot(cov_df, aes(Z, fill = interval)) +
  geom_histogram(color = "black", binwidth = 0.2) +
  theme_classic() +
  ggsci::scale_fill_jco() +
  labs(title = ("Distribution of Covariate")) +
  theme(plot.title = element_text(hjust = 0.5))
```

## Precision Matrix

All of the individuals in interval 1 had the same precision matrix, $\Omega^{(1)}$:

\[\Omega^{(1)}_{i,j} =
\begin{cases}
2 & i = j \\
1 & (i,j)\in\{(1, 2), (2,1),(2,3),(3,2)\} \\
0 & o.w.
\end{cases}\]

Also, all of the individuals in interval 3 had the same precision matrix, $\Omega^{(3)}$:

\[\Omega^{(3)}_{i,j} =
\begin{cases}
2 & i = j \\
1 & (i,j)\in\{(1,3), (3,1),(2,3),(3,2)\} \\
0 & o.w.
\end{cases}\]

However, the individuals in interval 2 had a precision matrix that was dependent upon $Z$ and $(a, b)$. Let $\beta_0 = -a / (b - a)$ and $\beta_1 = 1 / (b - a)$. Then:

\[\Omega^{(2)}_{i,j}(z) =
\begin{cases}
2 & i = j \\
1 & (i,j)\in\{(2,3), (3,2)\} \\
1 - \beta_0 - \beta_1z & (i,j)\in \{(1,2), (2,1)\} \\
\beta_0 + \beta_1z & (i,j)\in\{(1,3),(3,1)\}\\
0 & o.w.
\end{cases}
\]

Thus, $\Omega^{(2)}(a) = \Omega^{(1)}$ and $\Omega^{(2)}(b) = \Omega^{(3)}$. That is, an individual on the left or right boundary of $Z_2$ would have precision matrix $\Omega^{(1)}$ or $\Omega^{(3)}$, respectively. The conditional dependence structures corresponding to each of these precision matrices are visualized below.

```{r, echo = F, fig.width = 11, fig.height = 9}
# get the true graphs
true_graphs <- lapply(cont$true_precision, function(prec_mat) (prec_mat - diag(diag(prec_mat)) != 0) * 1)

# get the unique true graphs
strs <- unique(true_graphs)

# get the individual indices corresponding to each of the structures
ind_idx <- lapply(strs, function(strc) which(sapply(true_graphs, identical, strc)))

# get the summary for each
ind_sum <- sapply(ind_idx, function(idx) paste0(min(idx), ",...,", max(idx)))

# visualize each of the structures
str_viz <- lapply(1:length(strs), function(strc_idx)
  gg_adjMat(strs[[strc_idx]]) +
    ggtitle(paste("Individuals ", ind_sum[strc_idx])))
ggarrange(plotlist = str_viz)
```

## Data matrix

Let $z_l$ be the extraneous covariate for the $l$-th individual. To generate the data matrix for the $l$-th individual, I took a random sample from $\mathcal N (0,\{\Omega_l(z_l)\}^{-1})$, where: \[\Omega_l(z_l) =
\begin{cases}
\Omega^{(1)} & z_l\in Z_1 \\
\Omega^{(2)}(z_l) & z_l \in Z_2 \\
\Omega^{(3)} & z_l \in Z_3
\end{cases}
\]
