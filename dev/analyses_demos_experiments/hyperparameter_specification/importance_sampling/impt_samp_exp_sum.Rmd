---
title: "Untitled"
author: "Jacob Helwig"
date: "4/3/2022"
output: pdf_document
header-includes:
   - \usepackage{bbm, amsthm}
toc: T
---
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}

## Executive Summary

In this experiment, I compare a fully Bayesian model averaging approach to grid search. I find the performance of the model averaging approach to be significantly inferior to the grid search. Specifically, the model averaging results in the independence graph (a precision matrix of 0's) far too often.

Next steps will be to consider weightings calculated via the total ELBO across all $n$ weighted regressions, rather than individual specific sets of weights.

## Experiment Overview

This experiment compares three methods of hyperparameter specification for `covdepGE` across 10 trials. 

In each of the trials, a 1,000 point grid of hyperparameters was generated as described in the \textit{Grid generation} section. 

The first 2 methods (`prior` and `unif_prior`) employ a fully Bayesian approach of model averaging using importance weights, while the third method (`grid`) employs grid search.

`prior` placed non-uniform prior densities on each of the hyperparameters, while `unif_prior` used a uniform density for each. Both methods used a uniform importance density over each of the 1,000 grid points.

### `prior` prior specification

Although the prior placed on $\sigma^2$ was a flat prior, the other two hyperparameters had proper, non-uniform priors. 

First, the prior on $\pi$ was induced by a normal prior on the log-odds of $\pi$. The mean, -1.1, corresponded to $\pi = 0.2$, while the standard deviation, 1.1, was chosen so that the log-odds of $\pi = 0.5$ was within one standard deviation of the mean. This prior is visualized below:

```{r, echo = F}
library(ggplot2)

p <- 4

# function for calculating the log-odds
log_odds <- function(p) log(p / (1 - p))

# find the parameters of the prior on the log-odds:
mu_lo <- log_odds(1 / p)
sd_lo <- abs(log_odds((0.5 * p - 1) / p))

# plot the prior on PIP
ggplot() + geom_function(fun = function(x) dnorm(log_odds(x), mu_lo, sd_lo))
```
Next, the prior on $\sigma^2_\beta$ was induced by the prior proportion of variance explained. First, a uniform(0, 1) prior is placed on the prior proportion of variance explained, $\hat{r}$, which is defined by:

\begin{equation}
  \hat{r} = \frac{\hat{s_z}}{1 + \hat{s_z}} \quad\quad \hat{s_z} = \sigma^2_\beta * \pi * T \quad\quad T = \sum_{k = 1}^ps_k^2
\end{equation}

Where $s_k^2$ is the sample variance of the data matrix with the $k$-th column removed.

This prior, which is dependent on the prior value of $\pi$, induces the following prior on $\sigma^2_\beta$:

\begin{equation}
  \sigma^2_\beta \sim f(\cdot|\pi) \quad\quad f(\sigma^2_\beta|\pi) = \frac{\pi\sum_{k = 1}^ps_k^2}{(1 + \sigma^2_\beta\pi\sum_{k = 1}^ps_k^2)}\mathbbm{1}_{(0,\infty)}(\sigma^2_\beta)
\end{equation}

This prior is visualized below for $\pi$ = 0.2 and $T = 3$.

```{r, echo = F}
# function for calculating the prior density of sbsq given log-odds pi and the
# explanatory data
p_sbsq <- function(t, lo_pi, var_sum){
  
  # calculate the value of pi corresponding to the log-odds
  mu_pi <- exp(lo_pi) / (1 + exp(lo_pi))
  
  # calculate the density
  (pi * var_sum) / (1 + t * pi * var_sum)^2 * (t > 0)
}

ggplot() + geom_function(fun = p_sbsq, args = list(lo_pi = mu_lo, var_sum = 3), n = 1000) + xlim(c(1e-15, 1))
```

## Fully Bayesian Approach

This section describes the model averaging approach used here for a spike-and-slab regression of $y\in\mathbb R^n$ on $X\in\mathbb R^{n\times p}$ to compute an importance-weighted-average of the posterior inclusion probabilities $P(\gamma_k = 1|y, X, \theta_j)$, where $\gamma_k = 1 \iff \beta_k = 1$. 

Denote the importance weights by $\underset\sim w = w(\theta_1),..., w(\theta_N)$, where $\theta_1,..., \theta_N$ are points the points comprising the hyperparameter grid. 

With the $j$-th column of the data as the response $y$ and the data with the $j$-th column removed as the covariates $X$, an extraneous-covariate-weighted regression will be performed with respect to individual $l$ for each $\theta\in\underset\sim\theta$. The importance-weighted sum of the posterior quantities $p(\gamma_k = 1|X, y, \theta_i)$ will then be computed using: 

\begin{equation}
w(\theta_i) = \frac{p(y|X, \theta_i)p(\theta_i)}{q(\theta_i)}
\end{equation}

Where $p(y|X, \theta_i), p(\theta_i)$, and $q(\theta_i)$ are the marginal log-likelihood, the prior density, and the importance density, respectively. The prior density and the importance density are defined as: 

\begin{equation}
p(\theta_i) = p({\sigma^2}_i) * p({\sigma^2_\beta}_i) * p({\pi}_i) \quad\quad q(\theta_i) = q({\sigma^2}_i) * q({\sigma^2_\beta}_i) * q({\pi}_i)
\end{equation}

In the case of `prior`, $p({\sigma^2_\beta}_i)$ should actually be written hierarchically as $p({\sigma^2_\beta}_i|{\pi}_i)$. Also, in both `prior` and `unif_prior`, $q(\theta_i)$ is the uniform density on the hyperparameter grid. Then, the weighted importance sum coverges almost surely to the posterior probability of inclusion:

\begin{proposition}\label{as_pip}
Let $w(\theta_i) = p(y|X, \theta_i)$. Then: 
\begin{equation}
\lim_{N\to\infty}\frac{\sum_{i = 1}^N p(\gamma_k = 1|X, y, \theta_i)w(\theta_i)}{\sum_{i = 1}^N w(\theta_i)} = p(\gamma_k = 1|X, y), \textit{ a.s.}
\end{equation}
\end{proposition}

The approximation of $p(y|X, \theta_i)$ is made using $\exp(\textit{ELBO}_j^l(\theta_i))$, where $\textit{ELBO}_j^l(\theta_i)$ is the log-lower bound of the marginal likelihood $p(y|X,\theta_i)$ in the extraneous-covariate-weighted regression of $y$ on $X$ with respect to individual $l$.

## Grid generation

Let $X$ be the data with the $j$-th column removed, and $y$ be the $j$-th column of the data. This section describes how a grid of the following hyperparameters for a spike-and-slab regression of $y$ on $X$ is generated.

\begin{align*}
\sigma^2&\text{: residual error variance} \\
\sigma^2_\beta&\text{: regression coefficient variance}\\
\pi&\text{: prior probability of inclusion}
\end{align*}

Each of the hyperparameters is assumed to be bounded in the intervals: 

\begin{align*}
\sigma^2\in [0.00001,\underset{max}{\sigma^2}]\quad\quad\sigma^2_\beta\in [0.00001, \underset{max}{\sigma^2_\beta}]\quad\quad
\pi \in [0.01, \underset{max}{\pi}]
\end{align*}

The rough bound on $\sigma^2_\beta$ is induced by a bound on the signal to noise ratio of 25: 
\begin{equation}
  \underset{max}{\sigma^2_\beta} = \frac{25}{\hat\pi\sum_{k = 1}^p s_k^2}
\end{equation}

The bound on $\sigma^2$ is taken to be $\underset{max}{\sigma^2} = 10s_y^2$.

Then the grid for the hyperparameter $\theta$
is generated by first creating a uniform grid on $[\log\underset{min}{\theta}, \log\underset{max}{\theta}]$
and then exponentiating the resulting values. This results in more densely packed grid points on the lower side of the interval. For example, for $\sigma^2_\beta \in [1e^{-5}, 10]$, a grid of length 5 would be generated as:

```{r}
exp(seq(log(1e-3), log(10), length.out = 5))
```

Then, the final grid is created by taking the cartesian product between each of the individual grids: 

\begin{equation}
\mathcal S = \underset\sim{\sigma^2} \times \underset\sim{\sigma^2_\beta} \times \underset\sim{\pi}
\end{equation}

## Results

Detailed trial results are given below. It is readily apparent that a major shortcoming of the model averaging approach is that the sensitivity is too low. That is, `prior` and `unif_prior` produce graphs with no edges. 

It is unclear why this is occuring. A potential explanation is some mistake in the code. However, this seems unlikely, since the average ELBO for each of the models is displayed. This is calculated by summing the weighted average of the ELBO across the individuals. If there were some mistake made in the calculation of the weights, then the average ELBO would presumably be different than the ELBO for grid search - however, the ELBO of the averaged models is relatively close to that of the grid search.

## Potential Next Steps

Next steps could be to calculate weights at the level of the variable fixed as the response instead of the individual level. Then, the weights would be calculated the same as before, except instead of approximating $p(y|X, \theta_i)$ by $\exp(\textit{ELBO}_j^l(\theta_i))$, $p(y|X, \theta_i)$ could be approximated by $\exp(\textit{ELBO}_j(\theta_i))$, where:

\begin{equation}
  \exp(\textit{ELBO}_j(\theta_i)) = \sum_{l = 1}^n \exp(\textit{ELBO}^l_j(\theta_i))
\end{equation}

A potential issue with this is that it is unclear whether proposition 1 would still hold. 

## Detailed Results

```{r, echo = F}
library(covdepGE)
load("impt_samp_models.Rda")
#load("impt_samp_models_pipt2.Rda")
names(res) <- paste0("trial", 1:length(res))
mods <- lapply(res, `[[`, "models")
for (j in 1:length(mods)){
  cat(rep("-", 80), "\nTRIAL ", j, "\n\nprior\n")
  print(mods[[j]]$prior)
  print(plot(mods[[j]]$prior, title_sum = T))
  cat("\n\nTRIAL ", j, "\n\nunif_prior\n")
  print(mods[[j]]$unif_prior)
  print(plot(mods[[j]]$unif_prior, title_sum = T))
  cat("\n\nTRIAL ", j, "\n\ngrid\n")
  print(mods[[j]]$grid)
  print(plot(mods[[j]]$grid, title_sum = T))
}
```
