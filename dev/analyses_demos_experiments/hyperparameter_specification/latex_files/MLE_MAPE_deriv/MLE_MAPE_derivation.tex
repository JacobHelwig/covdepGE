%
% This is a borrowed LaTeX template file for lecture notes for CS267,
% Applications of Parallel Computing, UC Berkeley EECS Department.
% and Carnegie Mellon University's 10-725/36-725 
% Convex Optimization course taught by Ryan Tibshirani.  When
% preparing LaTeX notes for this class, please use this template. 

\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,graphicx,amssymb,amsthm,bbm}
\usepackage[mmddyy]{datetime}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor=blue}
%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
       \hbox to 6.28in { {\Large \hfill #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Author: #3 \hfill Created: #4} }
       \vspace{2mm}
       \hbox to 6.28in { {\it \hfill Last edit: \today} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{#2}{#2}

 %  {\bf Note}: {\it LaTeX template courtesy of UC Berkeley EECS dept \& CMU's convex optimization course taught by Ryan Tibshirani.}

}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
%\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

% -- some Macros defined by AB and DP
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\left<#1\right>}
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\nnorm}[1]{\big\Vert#1\big\Vert}
%\newcommand{\essnorm}[1]{\norm{#1}_{\ess}}
\newcommand\wt[1]{{ \widetilde{#1} }}

\newcommand \bbP{\mathbb{P}}
\newcommand \bbE{\mathbb{E }}
\newcommand \bbV{\mathbb{V}}
\newcommand \bbR{\mathbb{R}}
\newcommand \bbL{\mathbb{L}}
\newcommand \bft{\boldsymbol \theta}
\def\T{{ \mathrm{\scriptscriptstyle T} }}
\def\H{{ \mathrm{\scriptscriptstyle H} }}
\def\TV{{ \mathrm{\scriptscriptstyle TV} }}
\def\Gauss{{ \mathrm{N} }}
\def\d{{ \mbox{d} }}
\newcommand{\ind}{\mathbbm{1}}
\newcommand{\iu}{\mathrm{i}\mkern1mu}

\def\m{\mathcal}
\def\mb{\mathbb}
\def\mr{\mathrm}
\def\mx{\mbox}
\def\tr{{\rm tr\,}}
\newcommand{\1}{\\[1ex]}
\newcommand{\2}{\\[2ex]}
\newcommand{\3}{\\[3ex]}
\newcommand{\4}{\\[4ex]}

\DeclareMathOperator{\diag}{\text{diag}}
\DeclareMathOperator{\logit}{\text{logit}}
\DeclareMathOperator{\rank}{\text{rank}}

% -- some Macros defined by JH

% variance
\newcommand{\var}[1]{{\rm Var}\left(#1\right)}

% question environment
\newtheorem{q}[theorem]{Question}


\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{0}{Hyperparameter updates for \texttt{covdepGE}}{Jacob Helwig}{1/13/22}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.  
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.

\section{Notation}

For data $X\in\mathbb R^{n\times ({p + 1})}$, let $X_j$ be the $j$-th column of $X$, and $X_{-j}$ be X with the $j$-th column removed. Fix $l\in\{1,..,n\}$ and the $j$-the variable as the response $X_j$, and let $w_{i,l}$ be the weight of individual $i$ with respect to individual $l$. Then, $\beta_j^l$ denotes the weighted regression effect of $X_{-j}$ on $X_j$, and $\gamma^l_{j,k}$ is a Bernoulli random variable that takes on 1 if $\beta_{j,k}^l$ is non-zero. 

It is assumed that: 

\begin{equation}
x_{i,j}|\sigma^2, \beta_j^l\sim \mathcal N(x^\top_{i,-j}\beta_j^l, \sigma^2/w_{i,l}) \quad\quad \gamma_{j,k}^l\sim \text{Bern}(\pi)\quad\quad
\end{equation}

Thus, the weighted, conditional likelihood of $X_j$ for the $l$-th individual is given by: 

\begin{equation}
\label{likelihood}
L_l^w(j) \propto \prod_{i = 1}^n (\sigma^2)^{-1/2} \exp\left(-\frac{(x_{i,j} - x^\top_{i,-j}\beta_j^l)^2w_{i,l}}{2\sigma^2}\right)
\end{equation}

With the joint prior of $(\beta_j^l, \gamma_j^l)$:

\begin{equation}
\label{prior}
p_0(\beta_j^l,\gamma_j^l) = \prod_{k = 1}^{p - 1}\delta_{\{0\}}\left(\beta_{j,k}^l\right)^{1- \gamma_{j,k}^l}\mathcal N \left(\beta_{j,k}^l;0,\sigma^2\sigma^2_\beta\right)^{\gamma_{j,k}^l}\pi^{\gamma_{j,k}^l}(1-\pi)^{1-\gamma_{j,k}^l}
\end{equation}

\section{CAVI with hyperparameter updates}

The joint posterior of $(\beta_j^l,\gamma_j^l)$ is parameterized by $\mu_j^l$, $(s_j^l)^2$, and $\alpha^l_j$. $\mu_{j,k}^l$ and $(s_{j,k}^l)^2$ are the mean and variance of the Gaussian part of the marginal posterior distribution of $\beta_{j,k}^l$, and $\alpha_{j,k}^l$ is the probability that $\beta_{j,k}^l$ is non-zero. To produce a variational approximation to these parameters, CAVI is performed for fixed values of the hyperparameters $\sigma^2$, $\sigma^2_\beta$, and $\pi$.

The following sections detail how, for a fixed value of $\pi$, $\sigma^2$ and $\sigma^2_\beta$ may be updated during each CAVI iteration so that they are fit to the data. The update for $\sigma^2$ uses MLE, while the update for $\sigma^2_\beta$ uses maximum a posteriori estimation. 

These updates, as specified by \texttt{varbvs}, are provided in the last section. 

\section{MLE update for $\sigma^2$}

From \eqref{likelihood}, the weighted conditional log-likelihood is given by: 

\begin{equation}
\log(L_l^w(j))\propto \sum_{i = 1}^n -\log(\sigma^2) - \frac{(x_{i,j} - x^\top_{i,-j}\beta_j^l)^2w_{i,l}}{\sigma^2}
\end{equation}

Differentiating with respect to $\sigma^2$ gives:

\begin{equation}
\frac{\partial(\log(L_l^w(j)))}{\partial\sigma^2} \propto \sum_{i = 1}^n - \frac{1}{\sigma^2} + \frac{(x_{i,j} - x^\top_{i,-j}\beta_j^l)^2w_{i,l}}{(\sigma^2)^2}
\end{equation}

Setting equal to 0 and solving for $\sigma^2$ gives:

\begin{equation}
\sigma^2_{\textit{MLE}} = \frac1{n}\sum_{i = 1}^n (x_{i,j} - x^\top_{i,-j}\beta_j^l)^2w_{i,l}
\end{equation}

**Estimating the value of the parameter $\beta_j^l$ using its posterior expected value: 

\begin{equation}
\sigma^2_{\textit{MLE}} = \frac1{n}\sum_{i = 1}^n (x_{i,j} - x^\top_{i,-j}\mu_j^l)^2w_{i,l}
\end{equation}

\textit{**I don't believe that this is the correct reasoning}

\section{MAPE update for $\sigma^2_\beta$}

%The joint posterior of $(\beta_j^l,\gamma_j^l)$ is given by:
%
%\begin{equation}
%p(\beta_j^l,\gamma_j^l|X)\propto L_l^w(j)p_0(\beta_j^l, \gamma_j^l)
%\end{equation}
%
%Thus, the log posterior is given by:
%
%\begin{align}
%\log(p(\beta_j^l,\gamma_j^l|X)) & \propto \log(L_l^w(j)) + \log(p_0(\beta_j^l, \gamma_j^l))  \\
%& \propto \log(L_l^w(j)) + \log\left\{\prod_{k = 1}^{p - 1}\delta_{\{0\}}\left(\beta_{j,k}^l\right)^{1- \gamma_{j,k}^l}\pi^{\gamma_{j,k}^l}(1-\pi)^{1-\gamma_{j,k}^l}\right\}\\
%& \quad + \sum_{k = 1}^{p-1}\gamma_{j,k}^l\log(\frac1{\sqrt{2\pi\sigma^2}}) -\frac12\log(\sigma^2_\beta) \sum_{k = 1}^{p-1}\gamma_{j,k}^l - \frac12(\sigma^2\sigma^2_\beta)^{-1}\sum_{k=1}^{p-1}\gamma_{j,k}^l(\beta_{j,k}^l)^2
%\end{align}
%
%Differentiating with respect to $\sigma^2_\beta$ gives:
%
%\begin{equation}
%\frac{\partial\log(p(\beta_j^l,\gamma_j^l|X))}{\partial\sigma^2_\beta}\propto -\frac12(\sigma^2_\beta)^{-1} \sum_{k = 1}^{p-1}\gamma_{j,k}^l + \frac1{2\sigma^2}(\sigma^2_\beta)^{-2}\sum_{k=1}^{p-1}\gamma_{j,k}^l(\beta_{j,k}^l)^2
%\end{equation}
%
%Setting equal to 0 and solving for $\sigma^2_\beta$ gives:
%
%\begin{equation}
%(\sigma^2_\beta)_{\textit{MAPE}} = \frac{\sum_{k=1}^{p-1}\gamma_{j,k}^l(\beta_{j,k}^l)^2}{\sigma^2\sum_{k=1}^{p-1}\gamma_{j,k}^l}
%\end{equation}
%
%Approximating the value of the unknown parameters by their expected values** (that is, $\gamma_{j,k}^l$ with $\alpha_{j,k}^l$ and $\beta_{j,k}^l$ with $\mu_{j,k}^l$): 
%
%\textit{**Note: I am certain that this is not the correct reasoning for plugging in the variational approximations}
%
%\begin{equation}
%(\sigma^2_\beta)_{\textit{MAPE}} = \frac{\sum_{k=1}^{p-1}\alpha_{j,k}^l(\mu_{j,k}^l)^2}{\sigma^2\sum_{k=1}^{p-1}\alpha_{j,k}^l}
%\end{equation}
%
%\section{MAPE update for $\sigma_\beta^2$ with hyperprior}

\texttt{varbvs} imposes a scaled inverse chi-squared hyperprior on $\sigma^2_\beta$ with degrees of freedom $n_0$ and scale parameter $s_0$. That is:

\begin{equation}
\sigma^2_\beta \sim f_0(\sigma^2_\beta) \propto \frac{\exp\left(\frac{-n_0s_0}{2\sigma^2_\beta}\right)}{{(\sigma^2_\beta)^{1 + n_0/2}}}
\end{equation}

%This section is a copy of the previous section with the addition of this hyperprior. 

The joint posterior of $(\beta_j^l,\gamma_j^l)$ is given by:

\begin{equation}
p(\beta_j^l,\gamma_j^l|X)\propto L_l^w(j)p_0(\beta_j^l, \gamma_j^l)f_0(\sigma^2_\beta)
\end{equation}

Thus, the log posterior is given by:

\begin{align}
\log(p(\beta_j^l,\gamma_j^l|X)) & \propto \log(L_l^w(j)) + \log(p_0(\beta_j^l, \gamma_j^l))  \\
& \propto \log(L_l^w(j)) + \log\left\{\prod_{k = 1}^{p - 1}\delta_{\{0\}}\left(\beta_{j,k}^l\right)^{1- \gamma_{j,k}^l}\left(\frac1{\sqrt{\sigma^2}}\right)^{\gamma_{j,k}^l}\pi^{\gamma_{j,k}^l}(1-\pi)^{1-\gamma_{j,k}^l}\right\}\\
& \quad -\frac12\log(\sigma^2_\beta) \sum_{k = 1}^{p-1}\gamma_{j,k}^l - \frac12(\sigma^2\sigma^2_\beta)^{-1}\sum_{k=1}^{p-1}\gamma_{j,k}^l(\beta_{j,k}^l)^2 - \frac{n_0s_0}{2\sigma^2_\beta}-(1 + n_0/2)\log(\sigma^2_\beta)
\end{align}

Differentiating with respect to $\sigma^2_\beta$ gives:

\begin{equation}
\frac{\partial\log(p(\beta_j^l,\gamma_j^l|X))}{\partial\sigma^2_\beta}\propto -\frac12(\sigma^2_\beta)^{-1} \sum_{k = 1}^{p-1}\gamma_{j,k}^l + \frac1{2\sigma^2}(\sigma^2_\beta)^{-2}\sum_{k=1}^{p-1}\gamma_{j,k}^l(\beta_{j,k}^l)^2 +(\sigma^2_\beta)^{-2}\frac{n_0s_0}{2}-(1 + n_0/2)(\sigma^2_\beta)^{-1}
\end{equation}

Setting equal to 0 and solving for $\sigma^2_\beta$ gives:

\begin{equation}
(\sigma^2_\beta)_{\textit{MAPE}} = \frac{\sum_{k=1}^{p-1}\gamma_{j,k}^l(\beta_{j,k}^l)^2 + n_0s_0}{\sigma^2(2 + n_0 + \sum_{k=1}^{p-1}\gamma_{j,k}^l)}
\end{equation}

**Estimating the value of the parameters $\beta_j^l$ and $\gamma_j^l$ using their posterior expected values: 

\begin{equation}
(\sigma^2_\beta)_{\textit{MAPE}} = \frac{\sum_{k=1}^{p-1}\alpha_{j,k}^l(\mu_{j,k}^l)^2 + n_0s_0}{\sigma^2(2 + n_0 + \sum_{k=1}^{p-1}\alpha_{j,k}^l)}
\end{equation}

\textit{**I don't believe that this is the correct reasoning}

\section{\texttt{varbvs} hyperparameter updates}

Note that these updates are for an unweighted regression of $X$ onto $y$. These updates can be found in \href{https://github.com/pcarbo/varbvs/blob/master/varbvs-R/R/varbvsnorm.R}{this \texttt{varbvs} script}, under sections 2d and 2e. Unfamiliar helper functions, such as \texttt{dot} and \texttt{betavar} can be found in \href{https://github.com/pcarbo/varbvs/blob/master/varbvs-R/R/misc.R}{this \texttt{varbvs} script}.

Let $X_r = Xr$, where $r$ is a vector whose $k$-th entry is $\mu_{j,k}^l\alpha_{j,k}^l$ 

\begin{equation}
\sigma^2_{\textit{MLE}} = \frac{\lVert y - X_r\rVert^2_2 + \sum_{k = 1}^{p-1}\left\{(X_k^\top X_k)\alpha_{j,k}^l\left[(s_{j,k}^l)^2 + (1 - \alpha_{j,k}^l)(\mu_{j,k}^l)^2\right]\right\} + \frac1{\sigma^2_\beta}\sum_{k = 1}^{p - 1}\left\{\alpha_{j,k}^l((s_{j,k}^l)^2 + (\mu_{j,k}^l)^2)\right\}}{n + \sum_{k = 1}^{p - 1}\alpha_{j,k}^l}
\end{equation}

\begin{equation}
(\sigma^2_\beta)_{\textit{MAPE}} = \frac{s_0n_0 + \sum_{k = 1}^{p - 1}\alpha_{j,k}^l((s_{j,k}^l)^2 + (\mu_{j,k}^l)^2)}{n_0 + \sigma^2\sum_{k=1}^{p-1}\alpha_{j,k}^l}
\end{equation}



\end{document}
