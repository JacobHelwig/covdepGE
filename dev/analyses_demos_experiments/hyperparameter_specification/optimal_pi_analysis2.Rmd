---
title: "Optimal Pi and Importance Sampling Analysis"
output: pdf_document
toc: true
---

# Overview

In this document, I present the results of two experiments. The first describes the distribution of the optimal hyperparameters selected by grid search. In the second, I compare the results of a novel importance sampling - grid search hybrid scheme for hyperparameter specification. 

The first two main sections contain the results of these experiments, while the third describes the data and extraneous covariate generation.

# Optimal Pi 

## Hyperparameter specification

```{r, echo = F}
library(ggplot2)
library(ggpubr)
library(covdepGE)

#load("opt_pi2_5var.Rda")
load("opt_pi2_5var2.Rda")
# optimal pi analysis

# get all the models
mods <- lapply(res, `[[`, "out")

# recover the marginal hyperparameter grids
marg_grids <- apply(res[[1]]$out$hyperparameters$`Variable 1`$grid[ , 1:3], 2, 
                    unique)
```

In this experiment, I applied the grid search algorithm in 100 trials. I generated the grid from the cartesian product of the following marginal grids:

```{r}
# marginal grids
marg_grids

# number of grid points in the cartesian product
nrow(expand.grid(marg_grids))
```

## Optimal Hyperparameter Distribution

In this section, I display the distribution of optimal hyperparameters aggregated across all variables. Since a unique point in the hyperparameter grid was selected for each of the 5 variables, a total of 500 grid points are represented in each figure. 

I begin by displaying the true underlying precision structures to provide context for the optimal pi. 

```{r, echo = F, fig.width = 11, fig.height = 9}
set.seed(1)

# function for generating the data and the covariates
generate_continuous <- function(n1 = 60, n2 = 60, n3 = 60, p = 4){

  # create covariate for individuals in each of the three intervals

  # define the dimensions of the data
  n <- sum(n1, n2, n3)

  # define the limits of the intervals
  limits1 <- c(-3, -1)
  limits2 <- c(-1, 1)
  limits3 <- c(1, 3)

  # define the covariate values within each interval
  z1 <- runif(n1, limits1[1], limits1[2])
  z2 <- runif(n2, limits2[1], limits2[2])
  z3 <- runif(n3, limits3[1], limits3[2])
  Z <- matrix(sort(c(z1, z2, z3)), n, 1)

  # create precision matrices

  # the shared part of the structure for all three intervals is a 2 on the
  # diagonal and a 1 in the (2, 3) position
  common_str <- diag(p + 1)
  common_str[2, 3] <- 1

  # define constants for the structure of interval 2
  beta1 <- diff(limits2)^-1
  beta0 <- -limits2[1] * beta1

  # interval 2 has two different linear functions of Z in the (1, 2) position
  # and (1, 3) positions; define structures for each of these components
  int2_str12 <- int2_str13 <- matrix(0, p + 1, p + 1)
  int2_str12[1, 2] <- int2_str13[1, 3] <- 1

  # define the precision matrices for each of the individuals in interval 2
  int2_prec <- lapply(z2, function(z) common_str +
                        ((1 - beta0 - beta1*z)*int2_str12) +
                        ((beta0 + beta1*z)*int2_str13))

  # interval 1 has a 1 in the (1, 2) and interval 3 has a 1 in the (1, 3) position;
  # define structures for each of these components
  int1_str12 <- int3_str13 <- matrix(0, p + 1, p + 1)
  int1_str12[1, 2] <- int3_str13[1, 3] <- 1

  # define the precision matrices for each of the individuals in interval 1 and interval 3
  int1_prec <- rep(list(common_str + int1_str12), n1)
  int3_prec <- rep(list(common_str + int3_str13), n3)

  # put all of the precision matrices into one list
  prec_mats <- c(int1_prec, int2_prec, int3_prec)

  # symmetrize the precision matrices
  prec_mats <- lapply(prec_mats, function(mat) t(mat) + mat)

  # invert the precision matrices to get the covariance matrices
  cov_mats <- lapply(prec_mats, solve)

  # generate the data using the covariance matrices
  data_mat <- t(sapply(cov_mats, MASS::mvrnorm, n = 1, mu = rep(0, p + 1)))

  return(list(data = data_mat, covts = Z, true_precision = prec_mats))
}

# visualize the true precision structures
true_prec <- generate_continuous()$true_precision
true_graphs <- unique(lapply(true_prec, function(mat) (mat != 0) * 1 - diag(5)))
gr_viz <- lapply(true_graphs, gg_adjMat)
ggarrange(gr_viz[[1]] + ggtitle("Individuals 1,...,60"), 
          gr_viz[[2]] + ggtitle("Individuals 61,...,120"),
          gr_viz[[3]] + ggtitle("Individuals 121,...,180"))
```
```{r, echo = F, fig.width = 11, fig.height = 6}
# get all of the hyperparameter details from models
hyp <- lapply(mods, `[[`, "hyperparameters")

# get the optimal pi for each of the trials by variable
pi_opt <- lapply(hyp, lapply, `[[`, "pip")

# visualize the distribution of the optimal pi across all variables
pi_vec <- unlist(pi_opt)
pi_df <- cbind.data.frame(variable = names(pi_vec), pi = factor(pi_vec, levels = marg_grids$pip))
ggplot(pi_df, aes(pi, fill = variable)) + geom_bar(color = "black") + scale_x_discrete(drop = F) + 
  ggtitle("Distribution of Optimal Pi") +
  theme(axis.text.x = element_text(angle = 45, hjust=1))

# get the optimal sbsq for each of the trials by variable
sbsq_opt <- lapply(hyp, lapply, `[[`, "sbsq")

# visualize the distribution of the optimal sbsq across all variables
sbsq_vec <- unlist(sbsq_opt)
sbsq_df <- cbind.data.frame(variable = names(sbsq_vec), sbsq = factor(sbsq_vec, levels = marg_grids$sbsq))
ggplot(sbsq_df, aes(sbsq, fill = variable)) + geom_bar(color = "black") + scale_x_discrete(drop = F) + 
  ggtitle("Distribution of Optimal Sigmabeta_sq")

# get the optimal ssq for each of the trials by variable
ssq_opt <- lapply(hyp, lapply, `[[`, "ssq")

# visualize the distribution of the optimal ssq across all variables
ssq_vec <- unlist(ssq_opt)
ssq_df <- cbind.data.frame(variable = names(ssq_vec), ssq = factor(ssq_vec, levels = marg_grids$ssq))
ggplot(ssq_df, aes(ssq, fill = variable)) + geom_bar(color = "black") + scale_x_discrete(drop = F)+ 
  ggtitle("Distribution of Optimal Sigma_sq")
```

## Optimal Sigmabeta_sq as a function of Pi

Here, I visualize the optimal sigmabeta_sq as a function of pi. Note that the count is on the log scale. 

```{r, echo = F, fig.width = 11, fig.height = 6}
# visualize the optimal sbsq as a function of pi
(pi_tab <- table(pi_df$pi, sbsq_df$sbsq))
pitab_lg <- data.frame(expand.grid(
  pi = rownames(pi_tab), sbsq = colnames(pi_tab)), count = c(pi_tab))
pitab_lg$log_ct <- ifelse(pitab_lg$count == 0, 0, log(pitab_lg$count))
ggplot(pitab_lg, aes(pi, sbsq, fill = log_ct)) + 
  geom_tile(color = "black") + 
  scale_fill_gradient(low = "white") + 
  theme(axis.text.x = element_text(angle = 45, hjust=1))
# ggplot() + geom_jitter(aes(pi_vec, sbsq_vec), shape = 21, width = 0.005, height = 0.001, fill = "#80000075")
```

## Optimal Pi by variable

In this section, I visualize the optimal pi by variable. 

```{r, fig.width = 11, fig.height = 6, echo = F}
# visualize the optimal pi by variable
var_names <- names(hyp[[1]])
lapply(var_names, function(var_name) ggplot() +
         geom_bar(aes(factor(sapply(
           pi_opt, `[[`, var_name), levels = marg_grids$pip)), color = "black") +
         scale_x_discrete(drop = F) + xlab("pi") + ggtitle(var_name) + 
         theme(axis.text.x = element_text(angle = 45, hjust=1)))
```

## $p = 20$

In this section, I present the same analyses as above for a dataset with $p =20$.

```{r, echo = F}
load("opt_pi2__20var.Rda")
# optimal pi analysis

# get all the models
mods <- lapply(res, `[[`, "out")

# recover the marginal hyperparameter grids
marg_grids <- apply(res[[1]]$out$hyperparameters$`Variable 1`$grid[ , 1:3], 2, 
                    unique)
```

The marginal grids are:

```{r}
# marginal grids
marg_grids

# number of grid points in the cartesian product
nrow(expand.grid(marg_grids))
```

The true precision structures are given below:

```{r, echo = F, fig.width = 11, fig.height = 9}
set.seed(1)

# visualize the true precision structures
true_prec20 <- generate_continuous(p = 20 - 1)$true_precision
true_graphs <- unique(lapply(true_prec20, function(mat) (mat != 0) * 1 - diag(20)))
gr_viz <- lapply(true_graphs, gg_adjMat)
ggarrange(gr_viz[[1]] + ggtitle("Individuals 1,...,60"), 
          gr_viz[[2]] + ggtitle("Individuals 61,...,120"),
          gr_viz[[3]] + ggtitle("Individuals 121,...,180"))
```
Below is the distribution of the optimal hyperparameters aggregated across all variables.

```{r, echo = F, fig.width = 11, fig.height = 6}
# get all of the hyperparameter details from models
hyp <- lapply(mods, `[[`, "hyperparameters")

# get the optimal pi for each of the trials by variable
pi_opt <- lapply(hyp, lapply, `[[`, "pip")

# visualize the distribution of the optimal pi across all variables
pi_vec <- unlist(pi_opt)
pi_df <- cbind.data.frame(variable = names(pi_vec), pi = factor(pi_vec, levels = marg_grids$pip))
pi_df$variable <- factor(pi_df$variable)
varname_ord <- order(as.numeric(sapply(strsplit(
  levels(pi_df$variable), " "), `[[`, 2)))
pi_df$variable <- factor(pi_df$variable, levels(pi_df$variable)[varname_ord])
ggplot(pi_df, aes(pi, fill = variable)) + geom_bar(color = "black") + scale_x_discrete(drop = F) + 
  ggtitle("Distribution of Optimal Pi") +
  theme(axis.text.x = element_text(angle = 45, hjust=1))

# get the optimal sbsq for each of the trials by variable
sbsq_opt <- lapply(hyp, lapply, `[[`, "sbsq")

# visualize the distribution of the optimal sbsq across all variables
sbsq_vec <- unlist(sbsq_opt)
sbsq_df <- cbind.data.frame(variable = names(sbsq_vec), sbsq = factor(sbsq_vec, levels = marg_grids$sbsq))
sbsq_df$variable <- factor(sbsq_df$variable)
varname_ord <- order(as.numeric(sapply(strsplit(
  levels(sbsq_df$variable), " "), `[[`, 2)))
sbsq_df$variable <- factor(sbsq_df$variable, levels(sbsq_df$variable)[varname_ord])
ggplot(sbsq_df, aes(sbsq, fill = variable)) + geom_bar(color = "black") + scale_x_discrete(drop = F) + 
  ggtitle("Distribution of Optimal Sigmabeta_sq")

# get the optimal ssq for each of the trials by variable
ssq_opt <- lapply(hyp, lapply, `[[`, "ssq")

# visualize the distribution of the optimal ssq across all variables
ssq_vec <- unlist(ssq_opt)
ssq_df <- cbind.data.frame(variable = names(ssq_vec), ssq = factor(ssq_vec, levels = marg_grids$ssq))
ssq_df$variable <- factor(ssq_df$variable)
varname_ord <- order(as.numeric(sapply(strsplit(
  levels(ssq_df$variable), " "), `[[`, 2)))
ssq_df$variable <- factor(ssq_df$variable, levels(ssq_df$variable)[varname_ord])
ggplot(ssq_df, aes(ssq, fill = variable)) + geom_bar(color = "black") + scale_x_discrete(drop = F)+ 
  ggtitle("Distribution of Optimal Sigma_sq")
```

The optimal value of sbsq as a function of pi is visualized below.

```{r, echo = F, fig.width = 11, fig.height = 6}
# visualize the optimal sbsq as a function of pi
(pi_tab <- table(pi_df$pi, sbsq_df$sbsq))
pitab_lg <- data.frame(expand.grid(
  pi = rownames(pi_tab), sbsq = colnames(pi_tab)), count = c(pi_tab))
pitab_lg$log_ct <- ifelse(pitab_lg$count == 0, 0, log(pitab_lg$count))
ggplot(pitab_lg, aes(pi, sbsq, fill = log_ct)) + 
  geom_tile(color = "black") + 
  scale_fill_gradient(low = "white") + 
  theme(axis.text.x = element_text(angle = 45, hjust=1))
# ggplot() + geom_jitter(aes(pi_vec, sbsq_vec), shape = 21, width = 0.005, height = 0.001, fill = "#80000075")
```

Here the distribution of the optimal pi is visualized by variable.

```{r, fig.width = 11, fig.height = 6, echo = F}
# visualize the optimal pi by variable
var_names <- names(hyp[[1]])
lapply(var_names, function(var_name) ggplot() +
         geom_bar(aes(factor(sapply(
           pi_opt, `[[`, var_name), levels = marg_grids$pip)), color = "black") +
         scale_x_discrete(drop = F) + xlab("pi") + ggtitle(var_name) + 
         theme(axis.text.x = element_text(angle = 45, hjust=1)))
```

# Importance Sampling Hybrid

## Algorithm Overview

This algorithm averages over a deterministic grid of pi. For each value of pi, the values of sigmasq and sigmasq_beta are chosen using grid search. The importance weights are calculated by assigning a uniform prior to each value of pi and approximating the marginal likelihood using the ELBO.

## Hyperparameter Grid

Below, I display the grid of pi that are averaged over, as well as the marginal grids for sigmasq and sigmabeta_sq.

```{r, echo = F}
# importance sampling hybrid analysis
load("impt_hybrid2.Rda")

# get the importance and grid models
impt_mods <- lapply(res, `[[`, "impt")
grid_mods <- lapply(res, `[[`, "grid")

# recover the hyperparameter grids
marg_grid <- apply(grid_mods[[1]]$hyperparameters$`Variable 1`$grid[ , c("pip", "ssq", "sbsq")], 2, unique)

marg_grid
```

## Sensitivity and Specificity

In this section, I compare the sensitivity and the specificity for the two methods.

```{r, echo = F}
# get the true graphs
true_graphs <- lapply(true_prec, function(mat) (mat != 0) * 1 - diag(5))

# calculate total number of edges in the true graphs
num1 <- sum(unlist(true_graphs))
num0 <- sum(unlist(true_graphs) == 0)

# calculate sensitivity and specificity for models
impt1 <- sapply(1:length(impt_mods), function(trial_ind)
  sum(sapply(1:180, function(indv_ind)
    sum(impt_mods[[trial_ind]]$graphs[[indv_ind]] == 1 & true_graphs[[indv_ind]] == 1))))
impt0 <- sapply(1:length(impt_mods), function(trial_ind)
  sum(sapply(1:180, function(indv_ind)
    sum(impt_mods[[trial_ind]]$graphs[[indv_ind]] == 0 & true_graphs[[indv_ind]] == 0))))
impt_sens <- impt1 / num1
impt_spec <- impt0 / num0
grid1 <- sapply(1:length(grid_mods), function(trial_ind)
  sum(sapply(1:180, function(indv_ind)
    sum(grid_mods[[trial_ind]]$graphs[[indv_ind]] == 1 & true_graphs[[indv_ind]] == 1))))
grid0 <- sapply(1:length(grid_mods), function(trial_ind)
  sum(sapply(1:180, function(indv_ind)
    sum(grid_mods[[trial_ind]]$graphs[[indv_ind]] == 0 & true_graphs[[indv_ind]] == 0))))
grid_sens <- grid1 / num1
grid_spec <- grid0 / num0
```

```{r, fig.width = 11, fig.height = 6}
# grid sensitivity
summary(grid_sens)

# importance sensitivity
summary(impt_sens)

# difference in the sensitivities
summary(grid_sens - impt_sens)

# grid specificity
summary(grid_spec)

# importance specificity 
summary(impt_spec)

# difference in the specificities
summary(grid_spec - impt_spec)

# visualize the difference in the sensitivity
ggplot() + geom_histogram(aes(grid_sens - impt_sens), binwidth = 0.025, color = "black")
```

## Case Study

Here, I present the graphs estimated by the two methods in two cases; the first case represents the performance of the methods in the trial wherein the hybrid method had its maximum sensitivity. The second case represents the performance of the methods in the trial where the difference in the sensitivity of the two methods was maximal.

I also display the importance weights for variables 2 and 3 for individual 90 in both cases.

### Case 1: Optimal Importance Performance

```{r, fig.width = 11, fig.height = 9}
good_ind <- which.max(impt_sens)

# sensitivity for grid search
grid_sens[good_ind]

# sensitivity for importance sampling
impt_sens[good_ind]

# graphs estimated by grid search
ggarrange(plotlist = plot(res[[good_ind]]$grid))
```
```{r, fig.width = 11, fig.height = 9}
# graphs estimated by importance sampling
ggarrange(plotlist = plot(res[[good_ind]]$impt))
```

#### Importance Weights

```{r}
# visualize the weights for individual 90

# variable 2
res[[good_ind]]$impt$hyperparameters$`Variable 2`$weights[90, ]

# variable 3
res[[good_ind]]$impt$hyperparameters$`Variable 3`$weights[90, ]
```

### Case 2: Greatest Grid Search Margin

```{r, fig.width = 11, fig.height = 9}
diff_ind <- which.max(grid_sens - impt_sens)

# sensitivity for grid search
grid_sens[diff_ind]

# sensitivity for importance sampling
impt_sens[diff_ind]

# graphs estimated by grid search
ggarrange(plotlist = plot(res[[diff_ind]]$grid))

# graphs estimated by importance sampling
ggarrange(plotlist = plot(res[[diff_ind]]$impt))
```

#### Importance Weights

```{r}
# visualize the weights for individual 90

# variable 2
res[[diff_ind]]$impt$hyperparameters$`Variable 2`$weights[90, ]

# variable 3
res[[diff_ind]]$impt$hyperparameters$`Variable 3`$weights[90, ]
```

# Data Generation

## Extraneous Covariate

I generated the covariate, $Z$, as the union of three almost disjoint intervals of equal measure. That is, $Z = Z_1 \cup Z_2 \cup Z_3$ with $Z_1 = (-3, -1), Z_2 = (a,b) = (-1, 1), Z_3 = (1, 3)$. Within each interval, I generated 60 covariate values from a uniform distribution. For example: 

```{r, echo = F, fig.width = 11, fig.height = 4.5}
cont <- generate_continuous()
X <- cont$data
Z <- cont$covts
n <- nrow(X)
p <- ncol(X) - 1

interval <- c(rep(1, 60), rep(2, 60), rep(3, 60))
cov_df <- cbind.data.frame(interval = interval, Z, individual_index = 1:n)
cov_df$interval <- factor(cov_df$interval)
ggplot(cov_df, aes(Z, fill = interval)) +
  geom_histogram(color = "black", binwidth = 0.2) +
  theme_classic() +
  ggsci::scale_fill_jco() +
  labs(title = ("Distribution of Covariate")) +
  theme(plot.title = element_text(hjust = 0.5))
```

## Precision Matrix

All of the individuals in interval 1 had the same precision matrix, $\Omega^{(1)}$: 

\[\Omega^{(1)}_{i,j} = 
\begin{cases}
2 & i = j \\
1 & (i,j)\in\{(1, 2), (2,1),(2,3),(3,2)\} \\
0 & o.w.
\end{cases}\]

Also, all of the individuals in interval 3 had the same precision matrix, $\Omega^{(3)}$:

\[\Omega^{(3)}_{i,j} = 
\begin{cases}
2 & i = j \\
1 & (i,j)\in\{(1,3), (3,1),(2,3),(3,2)\} \\
0 & o.w.
\end{cases}\]

However, the individuals in interval 2 had a precision matrix that was dependent upon $Z$ and $(a, b)$. Let $\beta_0 = -a / (b - a)$ and $\beta_1 = 1 / (b - a)$. Then:

\[\Omega^{(2)}_{i,j}(z) = 
\begin{cases}
2 & i = j \\
1 & (i,j)\in\{(2,3), (3,2)\} \\
1 - \beta_0 - \beta_1z & (i,j)\in \{(1,2), (2,1)\} \\
\beta_0 + \beta_1z & (i,j)\in\{(1,3),(3,1)\}\\
0 & o.w.
\end{cases}
\]

Thus, $\Omega^{(2)}(a) = \Omega^{(1)}$ and $\Omega^{(2)}(b) = \Omega^{(3)}$. That is, an individual on the left or right boundary of $Z_2$ would have precision matrix $\Omega^{(1)}$ or $\Omega^{(3)}$, respectively. The conditional dependence structures corresponding to each of these precision matrices are visualized below.

```{r, echo = F, fig.width = 11, fig.height = 9}
# get the true graphs
true_graphs <- lapply(cont$true_precision, function(prec_mat) (prec_mat - diag(diag(prec_mat)) != 0) * 1)

# get the unique true graphs
strs <- unique(true_graphs)

# get the individual indices corresponding to each of the structures
ind_idx <- lapply(strs, function(strc) which(sapply(true_graphs, identical, strc)))

# get the summary for each 
ind_sum <- sapply(ind_idx, function(idx) paste0(min(idx), ",...,", max(idx)))

# visualize each of the structures
str_viz <- lapply(1:length(strs), function(strc_idx) 
  gg_adjMat(strs[[strc_idx]]) +
    ggtitle(paste("Individuals ", ind_sum[strc_idx])))
ggarrange(plotlist = str_viz)
```

## Data matrix

Let $z_l$ be the extraneous covariate for the $l$-th individual. To generate the data matrix for the $l$-th individual, I took a random sample from $\mathcal N (0,\{\Omega_l(z_l)\}^{-1})$, where: \[\Omega_l(z_l) = 
\begin{cases}
\Omega^{(1)} & z_l\in Z_1 \\
\Omega^{(2)}(z_l) & z_l \in Z_2 \\
\Omega^{(3)} & z_l \in Z_3
\end{cases}
\]
