---
title: "Hyperparameter specification experiment results"
output: pdf_document
---

# Summary

In comparing the current and a new method for specifying hyperparameters to `covdepGE`, I found that although the current method produces greater ELBO, the new method consistently outperforms the current one in terms of accuracy. I include visualizations for the differences in metrics, as well as a section analyzing the distribution of the candidate hyperparameters proposed from `varbvs`. 

# Experiment Overview

In this experiment, I compared two methods of hyperparameter specification across 500 trials. In each trial, the data was generated by setting a seed and then following the continuous data paradigm. That is, all of the individuals in the first and third interval of the covariate space had the same precision matrix ($\Omega_1$ and $\Omega_3$), while the precision matrix for individuals in the second interval continuously shifted from $\Omega_1$ to $\Omega_3$.

The first method (`M1`) follows the methodology for hyperparameter specification that we have already been utilizing. For each variable, a grid of 20 slab variances $\sigma^2_\beta$ are generated independent of the data and the other hyperparameters. The best $\sigma^2_\beta$ is selected based on ELBO. $\sigma^2$ is chosen by taking the mean across the candidates fit by `varbvs` and $\pi$ is chosen by taking the mean across the grid generated by `varbvs`. 

To demonstrate `M1`, below are the results for the first two variables from one trial. For each variable, the selected value of the hyperparameters are listed, followed by a value `hyperparameters` that lists each of the elements of the hyperparameter space for which a model was fit, along with the resulting `ELBO`.

```{r, echo = F, message = F}
library(covdepGE)
source("../generate_data.R")
cont <- generate_continuous(seed = 1)
X <- cont$data
Z <- cont$covts
```

```{r}
out1 <- covdepGE(X, Z, var_min = 1e-3, var_max = 1, n_sigma = 20, CS = T)
out1$CAVI_details[1:2]
```

The second method, `M2`, relies entirely on `varbvs` to generate the grid of hyperparameters by fitting $\sigma^2$ and $\sigma^2_\beta$ to the data for each value of $\pi$ in a grid of length 20. Again, the results for the first two variables are displayed.

```{r}
out2 <- covdepGE(X, Z)
out2$CAVI_details[1:2]
```

# Results: ELBO and accuracy

First, as a frame of reference for scale, the distribution of the ELBO, sensitivity, specificity, and accuracy for both `M1` and `M2` is visualized.

*Note: One of the ELBO for `M2` was removed, as it was on the order of * $1\times 10^{12}$. *However, all other metrics for this model were within 3% of that of `M1`, and so these were not dropped*

```{r, echo = F, message = F, fig.width = 11, fig.height = 9}
library(ggplot2)
library(ggpubr)
library(latex2exp)
library(tibble)

load("hyp_spec_exper_results.Rda")

M1_plots <- lapply(names(res$model1), function(metric)
  ggplot(data.frame(metric = res$model1[[metric]]), aes(metric)) +
    geom_histogram(color = "black", fill = "cadetblue3", bins = 50) +
    xlab(metric) + theme_bw() + 
    ggtitle(TeX(paste0("Distribution of ", metric, "_{M1}"))))
ggarrange(plotlist = M1_plots)

res_model2 <- res$model2
res_model2$elbo <- res_model2$elbo[-which.min(res_model2$elbo)]
M2_plots <- lapply(names(res_model2), function(metric)
  ggplot(data.frame(metric = res_model2[[metric]]), aes(metric)) +
    geom_histogram(color = "black", fill = "coral2", bins = 50) +
    xlab(metric) + theme_bw() + 
    ggtitle(TeX(paste0("Distribution of ", metric, "_{M2}"))))

ggarrange(plotlist = M2_plots)
```

Visualized next is the pairwise difference between the ELBO of `M1` and `M2`. The ELBO of `M1` was more frequently greater than that of `M2`

*Note: the large ELBO for `M2` was again removed*

```{r, echo = F, message = F}
# get the pairwise differences in each metric
metric_diffs <- lapply(1:length(res$model1), function(metric_ind) 
  res$model1[[metric_ind]] - res$model2[[metric_ind]])
names(metric_diffs) <- names(res$model1)

# visualize the distribution of the ELBO differences 

# remove the very large ELBO
metric_diffs$elbo <- metric_diffs$elbo[-which.max(metric_diffs$elbo)]

ggplot(tibble("ELBO Difference" = metric_diffs$elbo), aes(`ELBO Difference`)) + 
  geom_histogram(binwidth = 50, color = "black", fill = "seagreen") + 
  theme_bw() + 
  ggtitle(TeX("Distribution of ELBO_{M1} - ELBO_{M2}"))
summary(metric_diffs$elbo)
```

Visualized next is the pairwise differences in sensitivity, specificity, and accuracy.
Although the rate at which edges were correctly detected (sensitivity) was roughly equivalent between the methods, `M2` outperformed `M1` in non-edge detection rate (specificity), which resulted in the superior accuracy of `M2`.

```{r, echo = F}
# sensitivity
ggplot(tibble("Sensitivity Difference" = metric_diffs$sensitivity), aes(`Sensitivity Difference`)) + 
  geom_histogram(color = "black", fill = "deepskyblue3", bins = 35) + 
  theme_bw() + 
  ggtitle(TeX("Sensitivity_{M1} - Sensitivity_{M2}"))
summary(metric_diffs$sensitivity)

# specificity
ggplot(tibble("Specificity Difference" = metric_diffs$specificity), aes(`Specificity Difference`)) + 
  geom_histogram(color = "black", fill = "darkorchid3", bins = 35) + 
  theme_bw() + 
  ggtitle(TeX("Specificity_{M1} - Specificity_{M2}"))
summary(metric_diffs$specificity)

# accuracy
ggplot(tibble("Accuracy Difference" = metric_diffs$accuracy), aes(`Accuracy Difference`)) + 
  geom_histogram(color = "black", fill = "darkseagreen3", bins = 35) + 
  theme_bw() + 
  ggtitle(TeX("Accuracy_{M1} - Accuracy_{M2}"))
summary(metric_diffs$accuracy)
```

# Distribution of Candidates

The `pi` candidates generated by `varbvs` result from a grid of log-odds spanning from $-\log_{10}(p)$ to $-\log_{10}(1)$:

```{r}
p <- 4 
log.odds <- seq(-log10(p), -1, length.out = 20)
(pi <- 1 / (1 + 10^(-log.odds)))
mean(pi)
```

Since `M1` uses the mean of these candidates, all of the hyperparameter candidates for `M1` use $\pi \approx 0.14$, as can be seen from the demonstration of `M1` in the experiment overview section. However, `M2` fits a model for each of the candidates in the $\pi$ grid. 

The following is a visualization of the final values of $\pi$ chosen by the `M2` models. Since each variable can have its own value of $\pi$, and there are 5 variables per model, this distribution contains 2,500 values of $\pi$. This visualization shows that ELBO favors values of $\pi$ on the extremes of the distribution.

```{r, echo = F}
ggplot(tibble("Final Pi" = as.numeric(res$hyperparameters$pi_final)), aes(`Final Pi`)) + 
  geom_histogram(color = "black", fill = "chocolate", bins = 20) + 
  theme_bw() + ggtitle(TeX("Distribution of final $\\pi$")) + 
  xlab(TeX("Final $\\pi$"))
```

I proposed `M2` with the reasoning that since it fits $\sigma^2$ and $\sigma^2_\beta$ to the data for each $\pi$ in the grid, it would cover a greater portion of the hyperparameter space than `M1`. It is important to note that after analyzing the spread of $\sigma^2$ and $\sigma^2_\beta$ fit to each variable by `varbvs`, it became apparent that the candidates were relatively similar to one another. 

For reference to the magnitude of these candidates, here are summary statistics for the $\sigma^2$ candidates fit to the data by `varbvs`:

```{r, echo = F}
summary(as.numeric(res$hyperparameters$sigmasq_cands))
```

And the $\sigma^2_\beta$ candidates:

```{r}
summary(as.numeric(res$hyperparameters$sigmabetasq_cands))
```

The following visualizations are the distribution of the ranges of these candidates proposed by `varbvs`. For each variable in a model, `varbvs` fits $\sigma^2$ and $\sigma^2_\beta$ to the data for each of the 20 $\pi$ values in the grid. I defined the range as the difference between the largest and smallest hyperparameter fit to the data; since there were 500 models, and 5 variables per model, both of these distributions represent 2,500 ranges. 

<!-- ```{r, echo = F} -->
<!-- ggplot(tibble("Candidate sigma" = as.numeric(res$hyperparameters$sigmasq_cands)), aes(`Candidate sigma`)) +  -->
<!--   geom_histogram(color = "black", fill = "cornflowerblue", bins = 50) +  -->
<!--   theme_bw() + ggtitle(TeX("Distribution of candidate $\\sigma^2$")) +  -->
<!--   xlab(TeX("Candidate $\\sigma^2$")) -->

<!-- ggplot(tibble("Candidate sigmabetasq" = as.numeric(res$hyperparameters$sigmabetasq_cands)), aes(`Candidate sigmabetasq`)) +  -->
<!--   geom_histogram(color = "black", fill = "darkolivegreen3", bins = 50) +  -->
<!--   theme_bw() + ggtitle(TeX("Distribution of candidate $\\sigma^2_\\beta$")) +  -->
<!--   xlab(TeX("Candidate $\\sigma^2_\\beta$")) -->
<!-- ``` -->

```{r, echo = F}
ggplot(tibble("sigma range" = as.numeric(res$hyperparameters$sigmasq_ranges)), aes(`sigma range`)) + 
  geom_histogram(color = "black", fill = "forestgreen", bins = 20) + 
  theme_bw() + ggtitle(TeX("Distribution of ranges of $\\sigma^2$")) + 
  xlab(TeX("$\\sigma^2$ range"))
summary(as.numeric(res$hyperparameters$sigmasq_ranges))

ggplot(tibble("sigmabetasq range" = as.numeric(res$hyperparameters$sigmabetasq_ranges)), aes(`sigmabetasq range`)) + 
  geom_histogram(color = "black", fill = "darkturquoise", bins = 20) + 
  theme_bw() + ggtitle(TeX("Distribution of ranges of $\\sigma^2_\\beta$")) + 
  xlab(TeX("$\\sigma^2_\\beta$ range"))
summary(as.numeric(res$hyperparameters$sigmabetasq_ranges))
```
