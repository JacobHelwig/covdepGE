---
output: pdf_document
---

# `varbvs` overview

For $X'\in\mathbb{R}^{n\times p}$ and response $y\in\mathbb R^n$, the default algorithm for `varbvs` begins by initializing hyperparameters as follows: 

\begin{align*}
  \sigma^2 = \text{var}(y) \text{ (the error term variance)} \\
  \sigma^2_\beta = 1 \text{ (the slab variance)}\\
  \underset{\sim}{\frac{\pi}{1 - \pi}} \text{ is a vector of length twenty with equally spaced components ranging from }\frac{\pi_1}{1 - \pi_1} =- \log_{10}(p)\text{ to } \frac{\pi_{20}}{1 - \pi_{20}} = -1
\end{align*}

Each component of $\underset{\sim}{\frac{\pi}{1 - \pi}}$ is a candidate for the prior odds of inclusion. 

```{r, eval = F, include = F}
out <- varbvs(data, Z = NULL, y)
```

```{r, eval = F, include = F}
p <- ncol(data)

sigma <- rep(var(y), 20)

sa <- rep(1, 20)

# grid of 20 equally spaced points from -log10(p) to -1
log.odds <- seq(from = -log10(p), to = -1, length.out = 20) 
```

Next, for the $j$-th value in $\underset{\sim}{\frac{\pi}{1 - \pi}},\frac{\pi_j}{1 - \pi_j}$, CAVI is performed to obtain a variational estimate for $\mu_j$ (the mean of the non-zero regression coefficients) and $\alpha_j$ (the posterior inclusion probabilities).

At each step of the CAVI, after the variational update is performed, $\sigma^2_j$ and ${\sigma^2_\beta}_j$ are both updated using the current values of the variational parameters and hyperparameters. The update for $\sigma^2_j$ is performed using maximum likelihood estimation and the update for ${\sigma^2_\beta}_j$ is performed using maximum a posteriori estimation.

Finally, after the CAVI has concluded, the lower bound to the marginal log-liklihood given the hyperparameters, $\log(w_j)$ is calculated. 

```{r, eval = F, include = F}
# initialize variational parameters and storage for the lower bound
mu <- matrix(0, p, 20)
alpha <- matrix(0.1, p, 20)
logw <- rep(NA, 20)

for (i in 1:20){
  
  # CAVI loop
  for (j in 1:max_iter){
    
    # update alpha and mu
    var_update <- variational_update(sigma[i], sa[i], log.odds[i], alpha[ , i], mu[ , i])
    alpha[ , i] <- var_update$alpha
    mu[ , i] <- var_update$mu
    
    # compute the variational lower bound
    logw[i] <- variational_lowerbound(sigma[i], sa[i], log.odds[i], alpha[ , i], mu[ , i])
    
    # update sigma
    sigma[i] <- MLE_sigma(sigma[i], sa[i], alpha[ , i], mu[ , i])
    
    # update sa
    sa[i] <- MAPE_sa(sigma[i], sa[i], alpha[ , i], mu[ , i])
    
    # check whether the exit condition has been met
    if (exit_condition){
      break
    }
  }
  
}
```

Upon concluding this process for each of the values in $\underset{\sim}{\frac{\pi}{1 - \pi}}$, there will be 20 values of $\sigma^2_j$ and ${\sigma^2_\beta}_j$ corresponding to ${\frac{\pi_j}{1 - \pi_j}$ and having been fit to the data. Additionally, there will be 20 values of $\log(w_j)$.

The $k$-th value of the lower bounds is chosen such that $\log(w_k)$ maximizes the lower bound across the 20 recorded values. CAVI is then repeated for all of the values in $\underset{\sim}{\frac{\pi}{1 - \pi}}$, however, this time, the initial values for $\sigma^2_j$ and ${\sigma^2_\beta}_j$ are not $\text{var}(y)$ and $1$, but rather $\sigma^2_k$ and ${\sigma^2_\beta}_k$, respectively. 

Additionally, the starting values for $\mu_j$ and $\alpha_j$ are $\mu_k$ and $\alpha_k$. 

```{r, eval = F, include = F}
# find the index of the largest logw
max_index <- which.max(logw)

# select the initial values as those corresponding to the largest logw
sigma <- rep(sigma[max_index], 20)
sa <- rep(sa[max_index], 20)
alpha <- matrix(alpha[ , max_index], p, 20) # the max_index column of alpha repeated 20 times
mu <- matrix(mu[ , max_index], p, 20)

# repeat the loop from above 
for (i in 1:20){
  
  # CAVI loop
  for (j in 1:max_iter){
    
    # update alpha and mu
    var_update <- variational_update(sigma[i], sa[i], log.odds[i], alpha[ , i], mu[ , i])
    alpha[ , i] <- var_update$alpha
    mu[ , i] <- var_update$mu
    
    # compute the variational lower bound
    logw[i] <- variational_lowerbound(sigma[i], sa[i], log.odds[i], alpha[ , i], mu[ , i])
    
    # update sigma
    sigma[i] <- MLE_sigma(sigma[i], sa[i], alpha[ , i], mu[ , i])
    
    # update sa
    sa[i] <- MAPE_sa(sigma[i], sa[i], alpha[ , i], mu[ , i])
    
    # check whether the exit condition has been met
    if (exit_condition){
      break
    }
  }
  
}
```

As with the first CAVI, the values of $\sigma^2_j$ and ${\sigma^2_\beta}_j$ are updated at each iteration using maximum liklihood estimation and maximum a posteriori estimation. 

Upon completing CAVI for each of the $\frac{\pi_j}{1 - \pi_j}$, updated values are returned as $\underset{\sim}{\sigma^2}$ and $\underset{\sim}{\sigma^2_\beta}$, where the $j$-th value of this vector corresponds to $\frac{\pi_j}{1 - \pi_j}$.

Additionally, the posterior inclusion probabilities are calculated as $\underset\sim\alpha\underset\sim w$, where $\underset\sim w$ is the vector of normalized lower bounds and the $j$-th row of $\underset\sim\alpha$ is $\alpha_j$.

```{r, eval = F, include = F}
w <- normalizelogweights(logw)
PIP <- alpha %*% w
return(PIP, sigma, sa, log.odds)
```

In our algorithm, we define $y$ as the $i$-th column of $X$ and $X'$ as $X$ with the $i$-th column removed. We then use `varbvs` to obtain our hyperparameters for the $i$-th variable in the following manner:

We run the `varbvs` algorithm on $X'$ and $y$ as described above. To select hyperparameters, we set $\sigma^2 = \bar{\underset\sim{\sigma^2}}$ (sample mean of $\underset\sim{\sigma^2}$). We transform $\underset\sim{\frac\pi{1-\pi}}$ to $\underset\sim\pi$ and then set $\pi = \bar{\underset\sim\pi}$. We then define a grid of $\underset\sim{\sigma_\beta^2}$ independent of the results obtained from `varbvs` and independent of the data. We perform CAVI for each value in $\underset\sim{\sigma_\beta^2}$, selecting the final value for the $i$-th variable ${\sigma_\beta^2}_k$ to be the component of $\underset\sim{\sigma^2_\beta}$ that maximizes the ELBO. 

```{r, eval = F, include = F}
# list for storing the final models
final_models <- vector("list", p)

# slab variance candidates
sa <- c(0.01, 0.05, 0.1, 0.5, 1, 3, 7, 10)

for (i in 1:p){
  
  # fix the i-th column of the data as the response
  y <- X[ , i]
  
  # remove the i-th column from X
  data <- X[ , -i]
  
  # use varbvs to get hyperparamters
  out.varbvs <- varbvs(data, Z = NULL, y)
  sigma <- mean(out.varbvs$sigma)
  log.odds <- out.varbvs$log.odds
  probs <- 1 / (1 + 10^(-log.odds)) # transform the log odds to probabilities
  pi <- mean(probs)
  
  # list for storing models
  model_list <- vector("list", length(sa))
  
  # iterate over the slab variance candidates
  for (j in 1:length(sa)){
    
    # estimate the posterior inclusion probabilities
    model_list[[j]] <- covdepGE(y, X, sigma, sa[j], pi)

  }
  
  # find the model with the greatest ELBO and add it to the final models list
  final_models[[i]] <- get_best_model(model_list)

}

# using the posterior inclusion probabilties, get the graphs and return
graphs <- get_graphs(final_models)
return(graphs)
```

This approach seems inefficient for several reasons. First, it opts to reduce the values of $\underset\sim{\sigma^2}$ to $\bar{\underset\sim{\sigma^2}}$ and $\underset\sim{\pi}$ to $\bar{\underset\sim{\pi}}$ instead of exploring the different elements of the hyperparameter space $(\sigma^2_j, {\sigma^2_\beta}_j, \pi_j)$ that have been fit to the data in conjunction with one another. It also ignores entirely the values of $\underset\sim{\sigma^2_\beta}$ that have been fit to the data and instead generates a grid independent of the data and the other two hyperparameters.

Thus, a modification that could perhaps make our approach more effective would be to perform CAVI for each of the elements of the hyperparameter space $(\sigma^2_j, {\sigma^2_\beta}_j, \pi_j)$ returned by `varbvs` for the $i$-th variable and select the element $(\sigma^2_k, {\sigma^2_\beta}_k, \pi_k)$ that maximizes the ELBO.   

```{r, eval = F, include = F}
# list for storing the final models
final_models <- vector("list", p)

for (i in 1:p){
  
  # fix the i-th column of the data as the response
  y <- X[ , i]
  
  # remove the i-th column from X
  data <- X[ , -i]
  
  # use varbvs to get hyperparamters
  out.varbvs <- varbvs(data, Z = NULL, y)
  sigma <- out.varbvs$sigma
  sa <- out.varbvs$sa
  log.odds <- out.varbvs$log.odds
  probs <- 1 / (1 + 10^(-log.odds)) # transform the log odds to probabilities
  
  # list for storing models
  model_list <- vector("list", length(sa))
  
  # iterate over the hyperparameter tuples
  for (j in 1:length(log.odds)){
    
    # estimate the posterior inclusion probabilities
    model_list[[j]] <- covdepGE(y, X, sigma[j], sa[j], probs[j])

  }
  
  # find the model with the greatest ELBO and add it to the final models list
  final_models[[i]] <- get_best_model(model_list)

}

# using the posterior inclusion probabilties, get the graphs and return
graphs <- get_graphs(final_models)
return(graphs)
```

I believe that this approach would not only result in `covdepGE` utilizing more sensible values for $(\sigma^2_j, {\sigma^2_\beta}_j, \pi_j)$, but also would result in faster convergence, since the hyperparameters are fit to the data. Although it is true that these hyperparameters have been fit to the data under the assumption that the conditional dependence structure is homogeneous across all of the individuals, it seems that our current choice of hyperparameters is informed by the data even less. 

To explore whether this would actually result in an improvement, I will perform an experiment where I generate the continuous data multiple times. I will then call on `covdepGE` twice, first using the hyperparameter specification scheme that we are currently using, and then using the specification scheme that I suggest here. I will compare the distribution of the difference in the total model ELBO using each scheme across each data generation. I hypothesize that the scheme using the elements of the hyperparameter space suggested by `varbvs` will result in greater ELBO.
