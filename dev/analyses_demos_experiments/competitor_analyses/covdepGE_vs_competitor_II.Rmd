---
title: "covdepGE versus competitor II (non-sparse version)"
output: pdf_document
---

# Summary

In this competitor analysis, I generated 60 observations within each of the regions of the covariate space. Unlike the previous competitor analysis, the observations within each of the regions of the covariate space were not uniformally spaced. Rather, they were generated at random from a uniform distribution. 

After generating the covariate, I created the precision matrices dependent on the covariate and subsequently generated the data using the precision matrices. I then applied covariate dependent graphical modeling, as well as a covariate independent method. 

The independent method first clustered the observations based on the extraneous covariate before estimating a homeogeneous graph structure within each of the clusters. Clustering was performed using a Gaussian mixture model that selected the number of clusters based on the BIC. Homogeneous graphical estimation was performed by setting the covariate as a constant.

Hyperparameters were specified the same for both methods; $\pi = 0.1$, $\sigma^2$ as the mean of the residual variances fit to the unweighted data by `varbvs`, and $\sigma^2_\beta$ as chosen from a grid by maximizing ELBO. The grid was generated as: 

```{r}
n_param <- 50
slab_var_cands <- exp(seq(log(5), log(1e-3), length = n_param))
sprintf("%.1e", slab_var_cands)
```

I repeated this across 100 trials, generating the covariate and the data at random each time. I then compared the sensitivity, specificity, and accuracy of each of the methods. The final section includes detailed results for each of the trials.

## Sparisity

I repeated this experiment with sparsity. Instead of 60 individuals in the second interval of the covariate space, there were only 10. Results from both experiments are included in separate documents.

```{r}
sparse <- F
```

```{r, include = F}
start <- Sys.time()

# function for generating the data and the covariates
generate_continuous <- function(n1 = 60, n2 = 60, n3 = 60, p = 4){

  # create covariate for individuals in each of the three intervals
  
  # define the dimensions of the data
  n <- sum(n1, n2, n3)
  
  # define the limits of the intervals
  limits1 <- c(-3, -1)
  limits2 <- c(-1, 1)
  limits3 <- c(1, 3)
  
  # define the covariate values within each interval
  z1 <- runif(n1, limits1[1], limits1[2])
  z2 <- runif(n2, limits2[1], limits2[2])
  z3 <- runif(n3, limits3[1], limits3[2])
  # z1 <- seq(limits1[1], limits1[2], length = n1)
  # z2 <- seq(limits2[1], limits2[2], length = n2)
  # z3 <- seq(limits3[1], limits3[2], length = n3)
  Z <- matrix(sort(c(z1, z2, z3)), n, 1)
  
  # create precision matrices
  
  # the shared part of the structure for all three intervals is a 2 on the
  # diagonal and a 1 in the (2, 3) position
  common_str <- diag(p + 1)
  common_str[2, 3] <- 1
  
  # define constants for the structure of interval 2
  beta1 <- diff(limits2)^-1
  beta0 <- -limits2[1] * beta1
  
  # interval 2 has two different linear functions of Z in the (1, 2) position
  # and (1, 3) positions; define structures for each of these components
  int2_str12 <- int2_str13 <- matrix(0, p + 1, p + 1)
  int2_str12[1, 2] <- int2_str13[1, 3] <- 1
  
  # define the precision matrices for each of the individuals in interval 2
  int2_prec <- lapply(z2, function(z) common_str +
                        ((1 - beta0 - beta1*z)*int2_str12) +
                        ((beta0 + beta1*z)*int2_str13))
  
  # interval 1 has a 1 in the (1, 2) and interval 3 has a 1 in the (1, 3) position;
  # define structures for each of these components
  int1_str12 <- int3_str13 <- matrix(0, p + 1, p + 1)
  int1_str12[1, 2] <- int3_str13[1, 3] <- 1
  
  # define the precision matrices for each of the individuals in interval 1 and interval 3
  int1_prec <- rep(list(common_str + int1_str12), n1)
  int3_prec <- rep(list(common_str + int3_str13), n3)
  
  # put all of the precision matrices into one list
  prec_mats <- c(int1_prec, int2_prec, int3_prec)
  
  # symmetrize the precision matrices
  prec_mats <- lapply(prec_mats, function(mat) t(mat) + mat)
  
  # invert the precision matrices to get the covariance matrices
  cov_mats <- lapply(prec_mats, solve)
  
  # generate the data using the covariance matrices
  data_mat <- t(sapply(cov_mats, MASS::mvrnorm, n = 1, mu = rep(0, p + 1)))
  
  return(list(data = data_mat, covts = Z, true_precision = prec_mats))
}

library(covdepGE)
library(ggplot2)
library(latex2exp)
library(mclust)
library(ggpubr)

# colors for plots
set.seed(1)
colors <- c("chartreuse3", "chocolate2", "cornflowerblue", "darkgoldenrod1", 
            "darkmagenta", "deepskyblue3", "forestgreen", "darkorchid3", 
            "darkred", "darkslategray")
colors <- c(colors, sample(colors()[sapply(colors(), function(color) 
  !(substr(color, 1, 4)) %in% c("grey", "gray"))], 180))
```

# Data Generation

## Extraneous Covariate

I generated the covariate, $Z$, as the union of three almost disjoint intervals of equal measure. That is, $Z = Z_1 \cup Z_2 \cup Z_3$ with $Z_1 = (-3, -1), Z_2 = (a,b) = (-1, 1), Z_3 = (1, 3)$. Within each interval, I generated the covariate values from a uniform distribution. For example: 

```{r, echo = F, fig.width = 11, fig.height = 4.5}
n2 <- 60^(1 - sparse) * 10^sparse

cont <- generate_continuous(n2 = n2)
X <- cont$data
Z <- cont$covts
n <- nrow(X)
p <- ncol(X) - 1

interval <- c(rep(1, 60), rep(2, n2), rep(3, 60))
cov_df <- cbind.data.frame(interval = interval, Z, individual_index = 1:n)
cov_df$interval <- factor(cov_df$interval)
ggplot(cov_df, aes(Z, fill = interval)) +
  geom_histogram(color = "black", binwidth = 0.2) +
  theme_classic() +
  ggsci::scale_fill_jco() +
  labs(title = ("Distribution of Covariate")) +
  theme(plot.title = element_text(hjust = 0.5))
```

## Precision Matrix

All of the individuals in interval 1 had the same precision matrix, $\Omega^{(1)}$: 

\[\Omega^{(1)}_{i,j} = 
\begin{cases}
2 & i = j \\
1 & (i,j)\in\{(1, 2), (2,1),(2,3),(3,2)\} \\
0 & o.w.
\end{cases}\]

Also, all of the individuals in interval 3 had the same precision matrix, $\Omega^{(3)}$:

\[\Omega^{(3)}_{i,j} = 
\begin{cases}
2 & i = j \\
1 & (i,j)\in\{(1,3), (3,1),(2,3),(3,2)\} \\
0 & o.w.
\end{cases}\]

However, the individuals in interval 2 had a precision matrix that was dependent upon $Z$ and $(a, b)$. Let $\beta_0 = -a / (b - a)$ and $\beta_1 = 1 / (b - a)$. Then:

\[\Omega^{(2)}_{i,j}(z) = 
\begin{cases}
2 & i = j \\
1 & (i,j)\in\{(2,3), (3,2)\} \\
1 - \beta_0 - \beta_1z & (i,j)\in \{(1,2), (2,1)\} \\
\beta_0 + \beta_1z & (i,j)\in\{(1,3),(3,1)\}\\
0 & o.w.
\end{cases}
\]

Thus, $\Omega^{(2)}(a) = \Omega^{(1)}$ and $\Omega^{(2)}(b) = \Omega^{(3)}$. That is, an individual on the left or right boundary of $Z_2$ would have precision matrix $\Omega^{(1)}$ or $\Omega^{(3)}$, respectively. The conditional dependence structures corresponding to each of these precision matrices are visualized below.

```{r, echo = F, fig.width = 11, fig.height = 9}
# get the true graphs
true_graphs <- lapply(cont$true_precision, function(prec_mat) (prec_mat - diag(diag(prec_mat)) != 0) * 1)

# get the unique true graphs
strs <- unique(true_graphs)

# get the individual indices corresponding to each of the structures
ind_idx <- lapply(strs, function(strc) which(sapply(true_graphs, identical, strc)))

# get the summary for each 
ind_sum <- sapply(ind_idx, function(idx) paste0(min(idx), ",...,", max(idx)))

# visualize each of the structures
str_viz <- lapply(1:length(strs), function(strc_idx) 
  gg_adjMat(strs[[strc_idx]], color1 = colors[strc_idx]) +
    ggtitle(paste("Individuals ", ind_sum[strc_idx])))
ggarrange(plotlist = str_viz)
```

## Data matrix

Let $z_l$ be the extraneous covariate for the $l$-th individual. To generate the data matrix for the $l$-th individual, I took a random sample from $\mathcal N (0,\{\Omega_l(z_l)\}^{-1})$, where: \[\Omega_l(z_l) = 
\begin{cases}
\Omega^{(1)} & z_l\in Z_1 \\
\Omega^{(2)}(z_l) & z_l \in Z_2 \\
\Omega^{(3)} & z_l \in Z_3
\end{cases}
\]

```{r, echo = F}
# number of trials
n_trials <- 100

# list for storing the results of one of the methods on the j-th trial
results_j_method <- vector("list", 5)
names(results_j_method) <- c("summary", "unique_graphs", "sensitivity", "specificity", "accuracy")

# list for storing the covariates and the results of both methods on the j-th trial
results_j <- c(list(NULL), replicate(2, results_j_method, simplify = F))
names(results_j) <- c("covariates", "dependent", "independent")

# list for storing the results across n_trials trials
results <- replicate(n_trials, results_j, F)
names(results) <- paste0("trial", 1:n_trials)

# start the cluster
doParallel::registerDoParallel(5)

# perform n_trials trials
for (j in 1:n_trials){
  
  # generate the data
  cont <- generate_continuous(n2 = n2)
  X <- cont$data
  Z <- cont$covts
  
  # visualize the covariate
  cov_df <- cbind.data.frame(interval = interval, Z, individual_index = 1:n)
  cov_df$interval <- factor(cov_df$interval)
  cov_plot <- ggplot(cov_df, aes(Z, fill = interval)) +
    geom_histogram(color = "black", binwidth = 0.2) +
    theme_classic() +
    ggsci::scale_fill_jco() +
    labs(title = paste0("Distribution of Covariate, Trial ", j))
  results[[j]]$covariates <- cov_plot
  
  # covariate dependent 
  
  # use varbvs to get the hyperparameter sigma
  sigmasq <- mean(sapply(1:(p + 1), function(col_ind) mean(varbvs::varbvs(
    X[, -col_ind], Z, X[, col_ind], verbose = F)$sigma)))

  out_dep <- covdepGE(X, Z, sigmasq_vec = sigmasq, var_min = 1e-3, var_max = 5, 
                      n_param = n_param, parallel = T, stop_cluster = F, 
                      warnings = F)
  results[[j]]$dependent$summary <- out_dep
  results[[j]]$dependent$unique_graphs <- c(
    lapply(out_dep$unique_graphs, `[[`, "individuals_summary"), plot(out_dep, 
                                                                     colors, T))
  
  # covariate independent
  # apply Gaussian Mixture model clustering; selects number of clusters based on 
  # the model that results in the best BIC
  gmm <- Mclust(Z, verbose = F)
  
  # find number of clusters in final clustering
  num_clusters <- length(unique(gmm$classification))
  
  out_indep <- vector("list", num_clusters)
  
  # iterate over each of the clusters identified by GMM
  for (k in 1:num_clusters) {
    
    # fix the datapoints in the k-th cluster
    data_mat_k <- X[gmm$classification == k, ]
    
    # use varbvs to get the hyperparameter sigma
    sigmasq_k <- sapply(1:(p + 1), function(col_ind) mean(varbvs::varbvs(
      data_mat_k[, -col_ind], NULL, data_mat_k[, col_ind], verbose = F)$sigma))
    
    # apply the GGM using covdepGE with constant Z, save the resulting graph
    out_indep[[k]] <- covdepGE(data_mat_k, rep(0, nrow(data_mat_k)), 
                               sigmasq_vec = mean(sigmasq_k), var_min = 1e-3, 
                               var_max = 5, n_param = n_param, kde = F, 
                               scale = F, parallel = T, stop_cluster = F, 
                               warnings = F)
  }
  
  results[[j]]$independent$summary <- out_indep
  
  # get the graphs for each cluster
  adj_mats <- sapply(out_indep, function(out) unique(out$graphs))
  
  # find the individuals in each of the clusters
  clust_inds <- lapply(1:num_clusters, function(cl_ind) which(gmm$classification == cl_ind))
  clust_inds_sum <- lapply(clust_inds, function(indv_graph) paste0(sapply(
    split(sort(indv_graph), cumsum(c(1, diff(sort(indv_graph)) != 1))), function(
      idx_seq) ifelse(length(idx_seq) > 2, paste0(min(idx_seq), ",...,", max(
        idx_seq)), paste0(idx_seq, collapse = ","))), collapse = ","))
  names(clust_inds_sum) <- paste0("cluster", 1:length(clust_inds_sum))
  
  # create a list of n graphs according to the independent estimate for each 
  # cluster and the gmm cluster assignment for each individual
  indep_graphs <- sapply(1:n, function(ind_idx) adj_mats[gmm$classification[ind_idx]])
  
  # visualize the resulting graphs
  results[[j]]$independent$unique_graphs <- lapply(1:num_clusters, function(cl_ind) gg_adjMat(
    adj_mats[[cl_ind]], color1 = colors[cl_ind]) + labs(
      title = paste0("Cluster ", cl_ind, ": ", clust_inds_sum[cl_ind])))
  
  # add individual indices
  results[[j]]$independent$unique_graphs <- c(
    clust_inds_sum, results[[j]]$independent$unique_graphs)
  
  # find sensitivity, specificity, and accuracy for each method
  
  # find the true graph for each individual
  true_graphs <- lapply(cont$true_precision, function(mat) 
    ((mat - diag(diag(mat))) != 0) * 1)
  
  # find the total number of entries in all of the graphs
  true_gr_vector <- unlist(true_graphs)
  num_entries <- length(true_gr_vector)
  # p <- 4; num_entries == n * (p + 1)^2
  
  # find the total number of 1's and 0's in all the graphs
  tot1 <- sum(true_gr_vector)
  tot0 <- sum(-true_gr_vector + 1)
  # tot1 + tot0 == num_entries
  
  # create a list of lists; the j-th value in the outer list is a list with three 
  # values: the true, estimated (covariate dependent), and estimated (covariate 
  # independent) graphs for the j-th individual
  true_est_graphs <- lapply(1:n, function(ind_idx) list(
    true = true_graphs[[ind_idx]], est_dep = out_dep$graphs[[ind_idx]], 
    est_indep = indep_graphs[[ind_idx]]))
  
  # find the number of true positives for each method
  TP_dep <- sum(sapply(1:n, function(ind_idx) sum(
    (true_est_graphs[[ind_idx]]$true == 1) & (true_est_graphs[[ind_idx]]$est_dep == 1))))
  TP_indep <- sum(sapply(1:n, function(ind_idx) sum(
    (true_est_graphs[[ind_idx]]$true == 1) & (true_est_graphs[[ind_idx]]$est_indep == 1))))
  
  # find the number of true negatives for each method
  TN_dep <- sum(sapply(1:n, function(ind_idx) sum(
    (true_est_graphs[[ind_idx]]$true == 0) & (true_est_graphs[[ind_idx]]$est_dep == 0))))
  TN_indep <- sum(sapply(1:n, function(ind_idx) sum(
    (true_est_graphs[[ind_idx]]$true == 0) & (true_est_graphs[[ind_idx]]$est_indep == 0))))
  
  # find sensitivity, specificity, and accuracy for each method
  sensitivity_dep <- TP_dep / tot1
  sensitivity_indep <- TP_indep / tot1
  
  specificity_dep <- TN_dep / tot0
  specificity_indep <- TN_indep / tot0
  
  accuracy_dep <- (TN_dep + TP_dep) / num_entries
  accuracy_indep <- (TN_indep + TP_indep) / num_entries
  
  results[[j]]$dependent[c("sensitivity", "specificity", "accuracy")] <- 
    c(sensitivity_dep, specificity_dep, accuracy_dep)
  results[[j]]$independent[c("sensitivity", "specificity", "accuracy")] <- 
    c(sensitivity_indep, specificity_indep, accuracy_indep)
}

doParallel::stopImplicitCluster()
```

# Results

## Difference in performance

I first visualize the difference in each performance metric within each trial.

```{r, echo = F, fig.width = 11, fig.height = 4.5}
# get sensitivity, specificity, and accuracies for each trial and method
sens_dep <- sapply(results, function(results_j) results_j$dependent$sensitivity)
spec_dep <- sapply(results, function(results_j) results_j$dependent$specificity)
acc_dep <- sapply(results, function(results_j) results_j$dependent$accuracy)

sens_indep <- sapply(results, function(results_j)
  results_j$independent$sensitivity)
spec_indep <- sapply(results, function(results_j) 
  results_j$independent$specificity)
acc_indep <- sapply(results, function(results_j) results_j$independent$accuracy)

# get differences in each of the metrics
sens_diff <- sens_dep - sens_indep
spec_diff <- spec_dep - spec_indep
acc_diff <- acc_dep - acc_indep

# visualize the differences

# sensitivity
cat("\nSensitivity was greater for the covariate dependent method in ", sum(sens_diff > 0), "/", n_trials, " trials\n", sep = "")
summary(sens_diff)
ggplot(data.frame(x = sens_diff), aes(x)) + 
  geom_histogram(color = "black", fill = "cornflowerblue", binwidth = 0.025) + 
  ggtitle(TeX("Sensitivity_{dependent} - Sensitivity_{independent}")) +
  xlab("Difference in Sensitivity") + 
  theme_bw() +  
  theme(plot.title = element_text(hjust = 0.5))

# specificity
cat("\nSpecificity was greater for the covariate dependent method in ", sum(spec_diff > 0), "/", n_trials, " trials\n", sep = "")
summary(spec_diff)
ggplot(data.frame(x = spec_diff), aes(x)) + 
  geom_histogram(color = "black", fill = "goldenrod2", binwidth = 0.01) + 
  ggtitle(TeX("Specificty_{dependent} - Specificty_{independent}")) +
  xlab("Difference in Specificty") + 
  theme_bw() +  
  theme(plot.title = element_text(hjust = 0.5))

# accuracy
cat("\nAccuracy was greater for the covariate dependent method in ", sum(acc_diff > 0), "/", n_trials, " trials\n", sep = "")
summary(acc_diff)
ggplot(data.frame(x = acc_diff), aes(x)) + 
  geom_histogram(color = "black", fill = "darkviolet", binwidth = 0.01) + 
  ggtitle(TeX("Accuracy_{dependent} - Accuracy_{independent}")) +
  xlab("Difference in Accuracy") + 
  theme_bw() +  
  theme(plot.title = element_text(hjust = 0.5))
```

## Dependent performance 

Next, I visualize each of the metrics for the dependent method across all trials.

```{r, echo = F, fig.width = 11, fig.height = 4.5}
# specificity
ggplot(data.frame(x = spec_dep), aes(x)) + 
  geom_histogram(color = "black", fill = "cyan3", binwidth = 0.025) + 
  ggtitle(TeX("Specificty_{dependent}")) +
  xlab("Specificity") + 
  theme_bw() +  
  theme(plot.title = element_text(hjust = 0.5))
summary(spec_dep)

# sensitivity
ggplot(data.frame(x = sens_dep), aes(x)) + 
  geom_histogram(color = "black", fill = "darkorange2", binwidth = 0.01) + 
  ggtitle(TeX("Sensitivity_{dependent}")) +
  xlab("Sensitivity") + 
  theme_bw() +  
  theme(plot.title = element_text(hjust = 0.5))
summary(sens_dep)

# accuracy
ggplot(data.frame(x = acc_dep), aes(x)) + 
  geom_histogram(color = "black", fill = "forestgreen", binwidth = 0.01) + 
  ggtitle(TeX("Accuracy_{dependent}")) +
  xlab("Accuracy") + 
  theme_bw() +  
  theme(plot.title = element_text(hjust = 0.5))
summary(acc_dep)
```

# Detailed Trial Results

This section contains trial-by-trial details on the results and performance for each of the methods.

```{r, echo = F, fig.width = 3.5, fig.height = 2.5}
Sys.time() - start
results
```
